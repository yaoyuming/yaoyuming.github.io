<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>yahya的博客</title>
  
  <subtitle>学习笔记</subtitle>
  <link href="https://blog.yahyav2rayssr.top/atom.xml" rel="self"/>
  
  <link href="https://blog.yahyav2rayssr.top/"/>
  <updated>2023-04-16T09:37:11.778Z</updated>
  <id>https://blog.yahyav2rayssr.top/</id>
  
  <author>
    <name>Yahya</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>fastjson 常见问题</title>
    <link href="https://blog.yahyav2rayssr.top/posts/4e9562ed/"/>
    <id>https://blog.yahyav2rayssr.top/posts/4e9562ed/</id>
    <published>2023-04-16T09:37:11.778Z</published>
    <updated>2023-04-16T09:37:11.778Z</updated>
    
    <content type="html"><![CDATA[<h1 id="com-alibaba-fastjson-JSONException-syntax-error-expect-actual-string-pos-0-报错解决方案"><a href="#com-alibaba-fastjson-JSONException-syntax-error-expect-actual-string-pos-0-报错解决方案" class="headerlink" title="com.alibaba.fastjson.JSONException: syntax error, expect {, actual string, pos 0 报错解决方案"></a>com.alibaba.fastjson.JSONException: syntax error, expect {, actual string, pos 0 报错解决方案</h1><h2 id="解决方案1"><a href="#解决方案1" class="headerlink" title="解决方案1"></a>解决方案1</h2><p>用 <code>JSON.parse(String text)</code> 方法，去除转义即可：</p><p>把去除转义后的 字符串 传入 </p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java">JSONObject.parseObject(String text, Class&lt;T&gt; clazz);<br></code></pre></td></tr></table></figure><h2 id="解决方案2"><a href="#解决方案2" class="headerlink" title="解决方案2"></a>解决方案2</h2><p>利用字符串替换方法，替换掉转义的 \</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">String</span> <span class="hljs-variable">replaceText</span> <span class="hljs-operator">=</span> text.replaceAll(<span class="hljs-string">&quot;\\\\&quot;</span>, <span class="hljs-string">&quot;&quot;</span>);<br></code></pre></td></tr></table></figure><p>如果字符串前后还多了 “</p><p>再次切割</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">String</span> <span class="hljs-variable">paseText</span> <span class="hljs-operator">=</span> replaceText.substring(<span class="hljs-number">1</span>, replaceText.length() - <span class="hljs-number">1</span>);<br></code></pre></td></tr></table></figure><p>切割后，再次传入即可解决。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;com-alibaba-fastjson-JSONException-syntax-error-expect-actual-string-pos-0-报错解决方案&quot;&gt;&lt;a href=&quot;#com-alibaba-fastjson-JSONException-synt</summary>
      
    
    
    
    <category term="Java" scheme="https://blog.yahyav2rayssr.top/categories/Java/"/>
    
    <category term="工具库" scheme="https://blog.yahyav2rayssr.top/categories/Java/%E5%B7%A5%E5%85%B7%E5%BA%93/"/>
    
    
    <category term="fastjson" scheme="https://blog.yahyav2rayssr.top/tags/fastjson/"/>
    
  </entry>
  
  <entry>
    <title>fastjson 常用 API</title>
    <link href="https://blog.yahyav2rayssr.top/posts/86e940c9/"/>
    <id>https://blog.yahyav2rayssr.top/posts/86e940c9/</id>
    <published>2023-04-16T09:35:06.705Z</published>
    <updated>2023-04-16T09:35:06.705Z</updated>
    
    <content type="html"><![CDATA[<h1 id="对象和-json-互转"><a href="#对象和-json-互转" class="headerlink" title="对象和 json 互转"></a>对象和 json 互转</h1><h2 id="Object"><a href="#Object" class="headerlink" title="Object"></a>Object</h2><h3 id="转JSON对象"><a href="#转JSON对象" class="headerlink" title="转JSON对象"></a>转JSON对象</h3><blockquote><p><code>JSON</code> 是 <code>JSONObject</code> 的抽象类，<code>JSONObject</code>  共享<code>JSON</code> 类的 <code>toJson</code> 方法。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs java">  <span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br><span class="hljs-type">JSONObject</span> <span class="hljs-variable">json</span> <span class="hljs-operator">=</span> (JSONObject) JSON.toJSON(o);<br>      <span class="hljs-comment">// 或者</span><br>      <span class="hljs-type">JSONObject</span> <span class="hljs-variable">jsonObject</span> <span class="hljs-operator">=</span> (JSONObject) JSONObject.toJSON(stu);<br>  &#125;<br></code></pre></td></tr></table></figure><h2 id="JAVA对象"><a href="#JAVA对象" class="headerlink" title="JAVA对象"></a>JAVA对象</h2><h3 id="转JSON对象-1"><a href="#转JSON对象-1" class="headerlink" title="转JSON对象"></a>转JSON对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>    <span class="hljs-type">JSONObject</span> <span class="hljs-variable">jsonObject</span> <span class="hljs-operator">=</span> (JSONObject) JSONObject.toJSON(stu);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="转JSON字符串"><a href="#转JSON字符串" class="headerlink" title="转JSON字符串"></a>转JSON字符串</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>    <span class="hljs-type">String</span> <span class="hljs-variable">stuString</span> <span class="hljs-operator">=</span> JSONObject.toJSONString(stu);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="JSON对象"><a href="#JSON对象" class="headerlink" title="JSON对象"></a>JSON对象</h2><h3 id="转Java对象"><a href="#转Java对象" class="headerlink" title="转Java对象"></a>转Java对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>    <span class="hljs-type">Student</span> <span class="hljs-variable">stu</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">Student</span>(<span class="hljs-string">&quot;公众号编程大道&quot;</span>, <span class="hljs-string">&quot;m&quot;</span>, <span class="hljs-number">2</span>);<br><br>    <span class="hljs-comment">//先转成JSON对象</span><br>    <span class="hljs-type">JSONObject</span> <span class="hljs-variable">jsonObject</span> <span class="hljs-operator">=</span> (JSONObject) JSONObject.toJSON(stu);<br>    <span class="hljs-comment">//JSON对象转换成Java对象</span><br>    <span class="hljs-type">Student</span> <span class="hljs-variable">student</span> <span class="hljs-operator">=</span> JSONObject.toJavaObject(jsonObject, Student.class);<br>&#125;<br></code></pre></td></tr></table></figure><h2 id="JSON字符串"><a href="#JSON字符串" class="headerlink" title="JSON字符串"></a>JSON字符串</h2><h3 id="转JSON对象-2"><a href="#转JSON对象-2" class="headerlink" title="转JSON对象"></a>转JSON对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>    <span class="hljs-type">String</span> <span class="hljs-variable">stuString</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;&#123;\&quot;age\&quot;:2,\&quot;name\&quot;:\&quot;公众号编程大道\&quot;,\&quot;sex\&quot;:\&quot;m\&quot;&#125;&quot;</span>;<br>    <br>    <span class="hljs-comment">//JSON字符串转换成JSON对象</span><br>    <span class="hljs-type">JSONObject</span> <span class="hljs-variable">jsonObject1</span> <span class="hljs-operator">=</span> JSONObject.parseObject(stuString);<br>&#125;<br></code></pre></td></tr></table></figure><h3 id="转Java对象-1"><a href="#转Java对象-1" class="headerlink" title="转Java对象"></a>转Java对象</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-keyword">public</span> <span class="hljs-keyword">static</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">main</span><span class="hljs-params">(String[] args)</span> &#123;<br>    <span class="hljs-type">String</span> <span class="hljs-variable">stuString</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;&#123;\&quot;age\&quot;:2,\&quot;name\&quot;:\&quot;公众号编程大道\&quot;,\&quot;sex\&quot;:\&quot;m\&quot;&#125;&quot;</span>;<br><br>    <span class="hljs-comment">//JSON字符串转换成Java对象</span><br>    <span class="hljs-type">Student</span> <span class="hljs-variable">student1</span> <span class="hljs-operator">=</span> JSONObject.parseObject(stuString, Student.class);<br>    System.out.println(<span class="hljs-string">&quot;JSON字符串转换成Java对象\n&quot;</span> + student1);<br>&#125;<br></code></pre></td></tr></table></figure><h1 id="List-和-JSONArray-互转"><a href="#List-和-JSONArray-互转" class="headerlink" title="List 和 JSONArray 互转"></a>List 和 JSONArray 互转</h1><h2 id="List-转-JSONArray"><a href="#List-转-JSONArray" class="headerlink" title="List 转 JSONArray"></a>List 转 JSONArray</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java">List&lt;T&gt; list = <span class="hljs-keyword">new</span> <span class="hljs-title class_">ArrayList</span>&lt;T&gt;(); <br>JSONArray array= JSONArray.parseArray(JSON.toJSONString(list))；<br></code></pre></td></tr></table></figure><h2 id="JSONArray-转-List"><a href="#JSONArray-转-List" class="headerlink" title="JSONArray 转 List"></a>JSONArray 转 List</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">JSONArray</span> <span class="hljs-variable">array</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">JSONArray</span>(); <br>List&lt;Student&gt; list = JSONObject.parseArray(array.toJSONString(), Student.class);<br></code></pre></td></tr></table></figure><h2 id="字符串转-List"><a href="#字符串转-List" class="headerlink" title="字符串转 List"></a>字符串转 List</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">String</span> <span class="hljs-variable">str</span> <span class="hljs-operator">=</span> <span class="hljs-string">&quot;&quot;</span>; <br>List&lt;T&gt; list = JSONObject.parseArray(str,T.class);<br></code></pre></td></tr></table></figure><h1 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h1><h2 id="JSONField"><a href="#JSONField" class="headerlink" title="@JSONField"></a>@JSONField</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-meta">@JSONField(name=&quot;gender&quot;)</span> <br><span class="hljs-keyword">public</span> String sex;<br></code></pre></td></tr></table></figure><h2 id="JSONType"><a href="#JSONType" class="headerlink" title="@JSONType"></a>@JSONType</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//配置序列化的时候,不序列化id  sex</span><br><span class="hljs-meta">@JSONType(ignores =&#123;&quot;id&quot;, &quot;sex&quot;&#125;)</span> <br><span class="hljs-keyword">public</span> <span class="hljs-keyword">class</span> <span class="hljs-title class_">Person</span> <span class="hljs-keyword">implements</span> <span class="hljs-title class_">Serializable</span> &#123;&#125;<br></code></pre></td></tr></table></figure><h2 id="SerializeFilter"><a href="#SerializeFilter" class="headerlink" title="SerializeFilter"></a>SerializeFilter</h2><p>通过SerializeFilter可以使用扩展编程的方式实现定制序列化。fastjson提供了多种SerializeFilter：</p><h3 id="1-PropertyPreFilter"><a href="#1-PropertyPreFilter" class="headerlink" title="1. PropertyPreFilter"></a>1. PropertyPreFilter</h3><p>根据PropertyName判断是否序列化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-comment">//定制序列化,只序列化一部分字段,将需要序列化的字段名,配置到数组中 如果什么都不配置,则序列化全部字段</span><br>SimplePropertyPreFilterfilter = <span class="hljs-keyword">new</span> <span class="hljs-title class_">SimplePropertyPreFilter</span>(User.class, newString[]&#123;<span class="hljs-string">&quot;name&quot;</span>&#125;);<br><br><span class="hljs-type">String</span> <span class="hljs-variable">jsonString</span> <span class="hljs-operator">=</span>JSON.toJSONString(user,filter);<br></code></pre></td></tr></table></figure><h3 id="2-PropertyFilter"><a href="#2-PropertyFilter" class="headerlink" title="2. PropertyFilter"></a>2. PropertyFilter</h3><p>根据PropertyName和PropertyValue来判断是否序列化。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">PropertyFilter</span> <span class="hljs-variable">filter2</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">PropertyFilter</span>() &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title function_">apply</span><span class="hljs-params">(Object object, String key, Object value)</span> &#123;<br>        <span class="hljs-keyword">if</span> (key.equals(<span class="hljs-string">&quot;sex&quot;</span>)) &#123;<br>            <span class="hljs-keyword">if</span> ((Integer) value &gt; <span class="hljs-number">1</span>) &#123;<br>                <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>            &#125;<br>        &#125; <span class="hljs-keyword">else</span> <span class="hljs-keyword">if</span> (key.equals(<span class="hljs-string">&quot;name&quot;</span>)) &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;<br>        &#125;<br>        <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;<br>    &#125;<br>&#125;;<br><span class="hljs-type">String</span> <span class="hljs-variable">jsonString</span> <span class="hljs-operator">=</span> JSON.toJSONString(user, filter2);<br></code></pre></td></tr></table></figure><h3 id="3-NameFilter"><a href="#3-NameFilter" class="headerlink" title="3. NameFilter"></a>3. NameFilter</h3><p>修改Key，如果需要修改Key,process返回值则可。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs java">   <span class="hljs-comment">// 如果需要修改Key,process返回值则可</span><br><span class="hljs-comment">// 返回需要修改后的key值,如果不修改,则返回name,切记不能返回null,否则会报错</span><br>   <span class="hljs-type">NameFilter</span> <span class="hljs-variable">nameFilter</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">NameFilter</span>() &#123;<br>       <span class="hljs-meta">@Override</span><br>       <span class="hljs-keyword">public</span> String <span class="hljs-title function_">process</span><span class="hljs-params">(Object object, String name, Object value)</span> &#123;<br>           <span class="hljs-keyword">if</span> (name.equals(<span class="hljs-string">&quot;id&quot;</span>)) &#123;<br>               <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;ID&quot;</span>;<br>           &#125;<br><br>           <span class="hljs-keyword">return</span> name;<br>       &#125;<br>   &#125;;<br></code></pre></td></tr></table></figure><h3 id="4-ValueFilter"><a href="#4-ValueFilter" class="headerlink" title="4. ValueFilter"></a>4. ValueFilter</h3><p>修改Value。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">ValueFilter</span> <span class="hljs-variable">valueFilter</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">ValueFilter</span>() &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> Object <span class="hljs-title function_">process</span><span class="hljs-params">(Object object, String name, Object value)</span> &#123;<br>        <span class="hljs-keyword">if</span> (name.equals(<span class="hljs-string">&quot;name&quot;</span>)) &#123;<br>            <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;张三&quot;</span>;<br>        &#125;<br><br>        <span class="hljs-keyword">return</span> <span class="hljs-string">&quot;&quot;</span>;<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="5-BeforeFilter"><a href="#5-BeforeFilter" class="headerlink" title="5. BeforeFilter"></a>5. BeforeFilter</h3><p>序列化时在最前添加内容。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">BeforeFilter</span> <span class="hljs-variable">beforeFilter</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">BeforeFilter</span>() &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">writeBefore</span><span class="hljs-params">(Object object)</span> &#123;<br>        writeKeyValue(<span class="hljs-string">&quot;start&quot;</span>, <span class="hljs-string">&quot;bofore&quot;</span>);<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><h3 id="6-AfterFilter"><a href="#6-AfterFilter" class="headerlink" title="6. AfterFilter"></a>6. AfterFilter</h3><p>序列化时在最后添加内容。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs java"><span class="hljs-type">AfterFilter</span> <span class="hljs-variable">afterFilter</span> <span class="hljs-operator">=</span> <span class="hljs-keyword">new</span> <span class="hljs-title class_">AfterFilter</span>() &#123;<br>    <span class="hljs-meta">@Override</span><br>    <span class="hljs-keyword">public</span> <span class="hljs-keyword">void</span> <span class="hljs-title function_">writeAfter</span><span class="hljs-params">(Object object)</span> &#123;<br>        writeKeyValue(<span class="hljs-string">&quot;end&quot;</span>,<span class="hljs-string">&quot;after&quot;</span>);<br>    &#125;<br>&#125;;<br></code></pre></td></tr></table></figure><p></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;对象和-json-互转&quot;&gt;&lt;a href=&quot;#对象和-json-互转&quot; class=&quot;headerlink&quot; title=&quot;对象和 json 互转&quot;&gt;&lt;/a&gt;对象和 json 互转&lt;/h1&gt;&lt;h2 id=&quot;Object&quot;&gt;&lt;a href=&quot;#Object&quot; cla</summary>
      
    
    
    
    <category term="Java" scheme="https://blog.yahyav2rayssr.top/categories/Java/"/>
    
    <category term="工具库" scheme="https://blog.yahyav2rayssr.top/categories/Java/%E5%B7%A5%E5%85%B7%E5%BA%93/"/>
    
    
    <category term="fastjson" scheme="https://blog.yahyav2rayssr.top/tags/fastjson/"/>
    
  </entry>
  
  <entry>
    <title>JDK 安装</title>
    <link href="https://blog.yahyav2rayssr.top/posts/f4e001d9/"/>
    <id>https://blog.yahyav2rayssr.top/posts/f4e001d9/</id>
    <published>2023-04-16T09:08:07.797Z</published>
    <updated>2023-04-16T09:08:07.797Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CentOS-7-安装-JAVA环境（JDK-1-8）"><a href="#CentOS-7-安装-JAVA环境（JDK-1-8）" class="headerlink" title="CentOS 7 安装 JAVA环境（JDK 1.8）"></a>CentOS 7 安装 JAVA环境（JDK 1.8）</h1><h2 id="1-打开url选择jdk1-8下载"><a href="#1-打开url选择jdk1-8下载" class="headerlink" title="1. 打开url选择jdk1.8下载"></a>1. 打开url选择jdk1.8下载</h2><p><a href="https://www.oracle.com/java/technologies/downloads/#java8">https://www.oracle.com/java/technologies/downloads/#java8</a></p><p>这里选择的是 <a href="https://www.oracle.com/java/technologies/downloads/#license-lightbox">jdk-8u341-linux-x64.tar.gz</a></p> <img src="/posts/f4e001d9/image-20221010001751656.png" alt="image-20221010001751656" style="zoom: 50%;"><h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h2><p>创建安装目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">mkdir /usr/local/java<br></code></pre></td></tr></table></figure><p>解压至安装目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar -zxvf jdk-8u171-linux-x64.tar.gz -C /usr/local/java/<br></code></pre></td></tr></table></figure><h2 id="3-设置环境变量"><a href="#3-设置环境变量" class="headerlink" title="3. 设置环境变量"></a>3. 设置环境变量</h2><p>打开 <code>/etc/profile</code> 文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/profile<br></code></pre></td></tr></table></figure><p>在末尾添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">export JAVA_HOME=/usr/local/java/jdk1.8.0_212<br>export JRE_HOME=$&#123;JAVA_HOME&#125;/jre<br>export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib<br>export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH<br></code></pre></td></tr></table></figure><p>使环境变量生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profile<br></code></pre></td></tr></table></figure><p>添加软链接</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ln -s /usr/local/java/jdk1.8.0_212/bin/java /usr/bin/java<br></code></pre></td></tr></table></figure><p>检查</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">java -version<br></code></pre></td></tr></table></figure><img src="/posts/f4e001d9/image-20221010002813230.png" alt="image-20221010002813230" style="zoom:67%;"><p>然后使用 xsync 分发到另外两台机器上去，xsync 命令安装详见文章5。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;CentOS-7-安装-JAVA环境（JDK-1-8）&quot;&gt;&lt;a href=&quot;#CentOS-7-安装-JAVA环境（JDK-1-8）&quot; class=&quot;headerlink&quot; title=&quot;CentOS 7 安装 JAVA环境（JDK 1.8）&quot;&gt;&lt;/a&gt;CentO</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="JDK" scheme="https://blog.yahyav2rayssr.top/tags/JDK/"/>
    
  </entry>
  
  <entry>
    <title>HDFS 部署</title>
    <link href="https://blog.yahyav2rayssr.top/posts/beba2c3a/"/>
    <id>https://blog.yahyav2rayssr.top/posts/beba2c3a/</id>
    <published>2023-04-16T09:05:29.088Z</published>
    <updated>2023-04-16T09:05:29.088Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="hdfs界面设置操作文件"><a href="#hdfs界面设置操作文件" class="headerlink" title="hdfs界面设置操作文件"></a>hdfs界面设置操作文件</h2><p>在Hadoop的配置文件 core-site.xml 中增加如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;property&gt;<br>  &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;<br>  &lt;value&gt;hadoop&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;property&gt;<br>  &lt;name&gt;dfs.permissions.enabled&lt;/name&gt;<br>  &lt;value&gt;false&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://blog.csdn.net/qq_35246620/article/details/88576800">https://blog.csdn.net/qq_35246620/article/details/88576800</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;hdfs界面设置操作文件&quot;&gt;&lt;a href=&quot;#hdfs界面设置操作文件&quot; class=&quot;headerlink&quot; title=&quot;h</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="HDFS" scheme="https://blog.yahyav2rayssr.top/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>大数据安装部署（高可用版）</title>
    <link href="https://blog.yahyav2rayssr.top/posts/3ee3db16/"/>
    <id>https://blog.yahyav2rayssr.top/posts/3ee3db16/</id>
    <published>2023-04-16T09:03:38.829Z</published>
    <updated>2023-04-16T09:03:38.830Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><blockquote><p>Hadoop运行模式包括：本地模式、伪分布式模式以及完全分布式模式。(这里使用完全分布式模式)。</p></blockquote><p>准备3台虚拟机，最低要求：内存4G，硬盘40G，这里准备的虚拟机是4G，硬盘40G的配置。</p><p>机器配置如下：</p><table><thead><tr><th>hostname</th><th>ip</th><th>内存</th><th>cpu</th><th>磁盘</th></tr></thead><tbody><tr><td>node1</td><td>192.168.50.246</td><td>4G</td><td>2c</td><td>40G</td></tr><tr><td>node2</td><td>192.168.50.247</td><td>4G</td><td>2c</td><td>40G</td></tr><tr><td>node3</td><td>192.168.50.248</td><td>4G</td><td>2c</td><td>40G</td></tr></tbody></table><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><p>所有节点都需要配置。</p><blockquote><p>注意：安装 centos7 的时候如果是最小化安装（默认的选择就是最小化安装），是不安装 psmisc 包，此时 hadoop 的 HA 无法正常切换，需要安装 <code>yum install psmisc -y</code> 包后，重启。</p><p>说明一下：psmisc 工具包含了 pstree、killall、fuser</p><ul><li><p>pstree：以树状图显示程序。</p></li><li><p>killall：用于kill指定名称的进程。</p></li><li><p>fuser：用来显示所有正在使用着指定的file, file system 或者 sockets的进程信息。</p></li></ul></blockquote><h3 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h3><blockquote><p>后续操作都使用hadoop用户，不使用root用户进行操作。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">useradd hadoop<br>passwd hadoop<br></code></pre></td></tr></table></figure><p>然后配置用户具有root权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/sudoers<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># Allow root to run any commands anywhere</span></span><br>root  ALL=(ALL)     ALL<br>hadoop   ALL=(ALL)     ALL<br></code></pre></td></tr></table></figure><h3 id="机器时间同步"><a href="#机器时间同步" class="headerlink" title="机器时间同步"></a>机器时间同步</h3><p>安装依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo yum install -y epel-release<br>sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static<br></code></pre></td></tr></table></figure><p>时间同步（每台机器都要执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl start ntpd<br></code></pre></td></tr></table></figure><p>时间同步停止命令（不用执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop ntpd<br></code></pre></td></tr></table></figure><h3 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h3><p>每台机器设置各自的 hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">node1</span><br>sudo hostnamectl --static set-hostname node1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">node2</span><br>sudo hostnamectl --static set-hostname node2<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">node3</span><br>sudo hostnamectl --static set-hostname node3<br></code></pre></td></tr></table></figure><h3 id="设置host文件"><a href="#设置host文件" class="headerlink" title="设置host文件"></a>设置host文件</h3><p>所有机器都设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/hosts<br><br>192.168.50.246   node1<br>192.168.50.247   node2<br>192.168.50.248   node3<br></code></pre></td></tr></table></figure><h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo systemctl stop firewalld<br>sudo systemctl disable firewalld<br></code></pre></td></tr></table></figure><h3 id="创建文件夹并授权给hadoop用户"><a href="#创建文件夹并授权给hadoop用户" class="headerlink" title="创建文件夹并授权给hadoop用户"></a>创建文件夹并授权给hadoop用户</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo mkdir /app<br>sudo chown -R hadoop:hadoop /app<br></code></pre></td></tr></table></figure><h3 id="SSH免密登录配置"><a href="#SSH免密登录配置" class="headerlink" title="SSH免密登录配置"></a>SSH免密登录配置</h3><p>参考文章5</p><h3 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h3><blockquote><p>所有节点都要安装。</p></blockquote><p>参考文章2</p><h3 id="Zookeeper安装"><a href="#Zookeeper安装" class="headerlink" title="Zookeeper安装"></a>Zookeeper安装</h3><p>参考文章7</p><h3 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h3><blockquote><p>所有节点都要安装。</p></blockquote><p>参考文章3</p><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><h3 id="机器规划"><a href="#机器规划" class="headerlink" title="机器规划"></a>机器规划</h3><table><thead><tr><th>服务</th><th>node1</th><th>node2</th><th>node3</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>NameNode（active节点）</td><td>NameNode</td></tr><tr><td>HDFS</td><td>-</td><td>-</td><td>SecondaryNameNode</td></tr><tr><td>HDFS</td><td>DataNode</td><td>DateNode</td><td>DateNode</td></tr><tr><td>HDFS</td><td>JournalNode</td><td>JournalNode</td><td>JournalNode</td></tr><tr><td>YARN</td><td>ResourceManager</td><td>ResourceManager</td><td>-</td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr></tbody></table><h3 id="WEB-端口信息"><a href="#WEB-端口信息" class="headerlink" title="WEB 端口信息"></a>WEB 端口信息</h3><table><thead><tr><th>服务</th><th>类型</th><th>访问地址</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td><a href="http://node1:9870/">http://node1:9870</a>, <a href="http://node2:9870/">http://node2:9870</a>, <a href="http://node3:9870/">http://node3:9870</a></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td><a href="http://node3:9868/">http://node3:9868</a></td></tr><tr><td>YARN</td><td>jobhistory</td><td><a href="http://node3:19888/">http://node3:19888</a></td></tr><tr><td>YARN</td><td>ResourceManager</td><td><a href="http://node1:8088/">http://node1:8088</a>, <a href="http://node2:8088/">http://node2:8088</a></td></tr></tbody></table><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><blockquote><p>参考链接：<a href="https://blog.csdn.net/wjt199866/article/details/106473174">https://blog.csdn.net/wjt199866/article/details/106473174</a></p><p>更多配置参数信息，请参考官方网址查询</p><ul><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml</a></li><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></li><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></li><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></li></ul><p>通过这些网址，可以了解最新的全部的hadoop 配置信息，而且包括一些过时的定义标识，从而更好地维护您的集群。</p></blockquote><p>所有的配置文件都在 &#x2F;app&#x2F;hadoop-3.2.3&#x2F;etc&#x2F;hadoop 目录下，主要需要修改的配置文件如下：</p><h4 id="配置workers"><a href="#配置workers" class="headerlink" title="配置workers"></a>配置workers</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /app/hadoop-3.2.3/etc/hadoop/workers<br><br>node1<br>node2<br>node3<br></code></pre></td></tr></table></figure><h4 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">The java implementation to use. By default, this environment</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">variable is REQUIRED on ALL platforms except OS X!</span><br>export JAVA_HOME=/app/jdk1.8.0_212<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># To prevent accidents, shell commands be (superficially) locked</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">to only allow certain <span class="hljs-built_in">users</span> to execute certain subcommands.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">It uses the format of (<span class="hljs-built_in">command</span>)_(subcommand)_USER.</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># For example, to limit who can execute the namenode command,</span></span><br>export HDFS_NAMENODE_USER=&quot;hadoop&quot;<br>export HDFS_SECONDARYNAMENODE_USER=&quot;hadoop&quot;<br>export HDFS_DATANODE_USER=&quot;hadoop&quot;<br>export HDFS_JOURNALNODE_USER=&quot;hadoop&quot;<br>export HDFS_ZKFC_USER=&quot;hadoop&quot;<br>export YARN_RESOURCEMANAGER_USER=&quot;hadoop&quot;<br>export YARN_NODEMANAGER_USER=&quot;hadoop&quot;<br>export HADOOP_PID_DIR=/app/hadoop-3.2.3/tmp/hadoop-hadoop-datanode.pid<br></code></pre></td></tr></table></figure><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><blockquote><p>集群全局参数。</p><p>用于定义系统级别的参数，如 HDFS URL、Hadoop 的临时目录等。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 配置 hdfs 的地址，统一通信地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://vmcluster<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 整合 Zookeeper --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>ha.zookeeper.quorum<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:2181,node2:2181,node3:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 配置 hadoop 的数据目录 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/app/hadoop-3.2.3/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 配置 hadoop 的临时目录 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/app/hadoop-3.2.3/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 配置读写缓存大小 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>io.file.buffer.size<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>131072<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 代理用户配置 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- hdfs界面设置操作文件 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><blockquote><p>HDFS 参数。</p><p>如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/dfs/nn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/dfs/dn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.datanode-restart.timeout<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>30<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node3:9868<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 对照clickhouse，可以理解为给整个集群起的一个识别名字 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.nameservices<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>vmcluster<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- Currently, only a maximum of two NameNodes may be configured per nameservice. --&gt;</span><br>    <span class="hljs-comment">&lt;!-- unique identifiers for each NameNode in the nameservice --&gt;</span><br>    <span class="hljs-comment">&lt;!-- 目前为止，一个集群里面只能最多有两个NameNodes 注意了 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.namenodes.vmcluster<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>nn1,nn2,nn3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- the fully-qualified RPC address for each NameNode to listen on --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.vmcluster.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.vmcluster.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.rpc-address.vmcluster.nn3<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node3:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- the fully-qualified HTTP address for each NameNode to listen on --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.vmcluster.nn1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:9870<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.vmcluster.nn2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2:9870<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address.vmcluster.nn3<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node3:9870<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- the URI which identifies the group of JNs where the NameNodes will write/read edits --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>qjournal://node1:8485;node2:8485;node3:8485/vmcluster<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- the Java class that HDFS clients use to contact the Active NameNode --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.failover.proxy.provider.vmcluster<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <br>    <span class="hljs-comment">&lt;!-- a list of scripts or Java classes which will be used to fence the Active NameNode during a failover --&gt;</span><br>    <span class="hljs-comment">&lt;!-- 为了确保任何时候都只有一个NameNode在工作，failover的时候可能需要强制杀死一个NameNode，有两种方法，ssh或者shell，一般选择ssh --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>sshfence<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/home/hadoop/.ssh/id_rsa<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.fencing.ssh.connect-timeout<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>30000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.handler.count<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>100<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.safemode.threshold.pct<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.journalnode.edits.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/app/hadoop-3.2.3/data/jn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 启用自动故障转移 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.replication<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.blocksize<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>67108864<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h4><blockquote><p>Mapreduce 参数。</p><p>包括 JobHistory Server 和应用程序参数两部分，如 reduce 任务的默认个数、任务所能够使用内存的默认上下限等。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 历史服务器端地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node3:10020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 历史服务器web端地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node3:19888<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/app/hadoop-3.2.3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.map.env<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/app/hadoop-3.2.3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.reduce.env<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/app/hadoop-3.2.3<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.application.classpath<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><blockquote><p>集群资源管理系统参数。</p><p>配置 ResourceManager，NodeManager 的通信端口，web监控端口等。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarnCluster<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.automatic-failover.embedded<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.connect.retry-interval.ms<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>2000<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>rm1,rm2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:8088<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.webapp.address.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2:8088<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.address.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:8032<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.address.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2:8032<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm1<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:8030<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address.rm2<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2:8030<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 整合 Zookeeper --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:2181,node2:2181,node3:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce_shuffle.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 日志采集 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log.server.url<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>http://node3:19888/jobhistory/logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>604800<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.store.class<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-comment">&lt;!-- 整合 Zookeeper --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.zk.state-store.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:2181,node2:2181,node3:2181<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.application.classpath<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span><br>            $HADOOP_CONF_DIR,<br>            $HADOOP_COMMON_HOME/share/hadoop/common/*,<br>            $HADOOP_COMMON_HOME/share/hadoop/common/lib/*,<br>            $HADOOP_HDFS_HOME/share/hadoop/hdfs/*,<br>            $HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*,<br>            $HADOOP_YARN_HOME/share/hadoop/yarn/*,<br>            $HADOOP_YARN_HOME/share/hadoop/yarn/lib/*<br>        <span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapred.job.queue.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop.myqueue<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="分发配置文件"><a href="#分发配置文件" class="headerlink" title="分发配置文件"></a>分发配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">xsync /app/hadoop-3.2.3/etc/hadoop<br></code></pre></td></tr></table></figure><h2 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h2><h3 id="zk-初始化"><a href="#zk-初始化" class="headerlink" title="zk 初始化"></a>zk 初始化</h3><p>在 node1 上格式化 zookeeper，第33行的日志表示创建成功。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs zkfc -formatZK<br></code></pre></td></tr></table></figure><p>验证 zkfc 是否格式化成功，如果多了一个 hadoop-ha 包就是成功了，如下所示：</p><img src="/posts/3ee3db16/image-20221016231450173.png" alt="image-20221016231450173" style="zoom:67%;"><h3 id="启动-JournalNode-集群"><a href="#启动-JournalNode-集群" class="headerlink" title="启动 JournalNode 集群"></a>启动 JournalNode 集群</h3><p>依次在 node1, node2, node3 上面执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start journalnode<br></code></pre></td></tr></table></figure><h3 id="NameNode-集群初始化"><a href="#NameNode-集群初始化" class="headerlink" title="NameNode 集群初始化"></a>NameNode 集群初始化</h3><p>格式化集群的一个NameNode（node1），有两种方法，我使用的是第一种：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs namenode –format<br></code></pre></td></tr></table></figure><p>在 node1 上启动刚才格式化的 namenode：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start namenode<br></code></pre></td></tr></table></figure><p>在 node1 机器上，将 node1 的数据复制到 node2 上来,在 node 2 上执行（node3 同理）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs namenode –bootstrapStandby<br></code></pre></td></tr></table></figure><p>启动 node2 和 node3 的 namenode：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start namenode<br></code></pre></td></tr></table></figure><h3 id="DataNode-启动"><a href="#DataNode-启动" class="headerlink" title="DataNode 启动"></a>DataNode 启动</h3><p>启动所有的datanode，在 node1 上执行：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start datanode<br></code></pre></td></tr></table></figure><h3 id="Yarn-启动"><a href="#Yarn-启动" class="headerlink" title="Yarn 启动"></a>Yarn 启动</h3><p>启动yarn，在 node1 上执行以下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">start-yarn.sh<br></code></pre></td></tr></table></figure><h3 id="测试-HDFS-是否可用"><a href="#测试-HDFS-是否可用" class="headerlink" title="测试 HDFS 是否可用"></a>测试 HDFS 是否可用</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs dfs -ls /<br></code></pre></td></tr></table></figure><h2 id="一键启动"><a href="#一键启动" class="headerlink" title="一键启动"></a>一键启动</h2><p>在 node1 机器下的 <code>/app/hadoop-3.2.3/sbin</code> 目录下执行一键启动命令（需提前启动好 zookeeper 集群）：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">start-all.sh<br></code></pre></td></tr></table></figure><h2 id="历史服务器配置"><a href="#历史服务器配置" class="headerlink" title="历史服务器配置"></a>历史服务器配置</h2><p>主要对应配置文件 mapred-site.xml，增加如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;!-- 历史服务器端地址 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br>    &lt;value&gt;node3:10020&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 历史服务器web端地址 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;<br>    &lt;value&gt;node3:19888&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><p>启动停止（node3 机器执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动</span><br>./bin/mapred --daemon start historyserver<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">停止</span><br>./bin/mapred --daemon stop historyserver<br></code></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://blog.51cto.com/mapengfei/4778140">YARN &amp;&amp; Hadoop 集群环境准备</a></p><p><a href="http://liangfan.tech/2019/03/15/Hadoop%E4%B9%8B4-HDFS%E9%AB%98%E5%8F%AF%E7%94%A8%E9%85%8D%E7%BD%AE/">Hadoop之4-HDFS HA配置</a></p><p><a href="https://blog.csdn.net/qq_35029061/article/details/125810621">Hadoop HA 高可用集群搭建</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据常见错误：HDFS报错解决：Operation category JOURNAL is not supported in state standby</title>
    <link href="https://blog.yahyav2rayssr.top/posts/d8e7a818/"/>
    <id>https://blog.yahyav2rayssr.top/posts/d8e7a818/</id>
    <published>2023-04-16T09:02:51.185Z</published>
    <updated>2023-04-16T09:02:51.185Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="HDFS报错解决：Operation-category-JOURNAL-is-not-supported-in-state-standby"><a href="#HDFS报错解决：Operation-category-JOURNAL-is-not-supported-in-state-standby" class="headerlink" title="HDFS报错解决：Operation category JOURNAL is not supported in state standby"></a>HDFS报错解决：Operation category JOURNAL is not supported in state standby</h2><h3 id="现象"><a href="#现象" class="headerlink" title="现象"></a>现象</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br></pre></td><td class="code"><pre><code class="hljs shell">org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error<br>at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:88)<br>at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1954)<br>at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1442)<br>at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4716)<br>at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1293)<br>at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:148)<br>at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:14726)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)<br>at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)<br>at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)<br>at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)<br>at java.security.AccessController.doPrivileged(Native Method)<br>at javax.security.auth.Subject.doAs(Subject.java:422)<br>at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)<br>at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)<br> <br>at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)<br>at org.apache.hadoop.ipc.Client.call(Client.java:1457)<br>at org.apache.hadoop.ipc.Client.call(Client.java:1367)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)<br>at com.sun.proxy.$Proxy16.rollEditLog(Unknown Source)<br>at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:152)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:365)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:362)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:504)<br>at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>at java.lang.Thread.run(Thread.java:748)<br>2019-06-28 03:27:27,782 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nns, ipcAddress=nns/192.168.56.14:8020, httpAddress=http://nns:50070], try next.<br>org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error<br>at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:88)<br>at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1954)<br>at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1442)<br>at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4716)<br>at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1293)<br>at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:148)<br>at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:14726)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)<br>at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)<br>at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)<br>at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)<br>at java.security.AccessController.doPrivileged(Native Method)<br>at javax.security.auth.Subject.doAs(Subject.java:422)<br>at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)<br>at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)<br> <br>at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)<br>at org.apache.hadoop.ipc.Client.call(Client.java:1457)<br>at org.apache.hadoop.ipc.Client.call(Client.java:1367)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)<br>at com.sun.proxy.$Proxy16.rollEditLog(Unknown Source)<br>at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:152)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:365)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:362)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:504)<br>at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>at java.lang.Thread.run(Thread.java:748)<br>2019-06-28 03:27:27,829 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Exception from remote name node RemoteNameNodeInfo [nnId=nns, ipcAddress=nns/192.168.56.14:8020, httpAddress=http://nns:50070], try next.<br>org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.StandbyException): Operation category JOURNAL is not supported in state standby. Visit https://s.apache.org/sbnn-error<br>at org.apache.hadoop.hdfs.server.namenode.ha.StandbyState.checkOperation(StandbyState.java:88)<br>at org.apache.hadoop.hdfs.server.namenode.NameNode$NameNodeHAContext.checkOperation(NameNode.java:1954)<br>at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation(FSNamesystem.java:1442)<br>at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:4716)<br>at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1293)<br>at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:148)<br>at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:14726)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)<br>at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)<br>at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)<br>at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)<br>at java.security.AccessController.doPrivileged(Native Method)<br>at javax.security.auth.Subject.doAs(Subject.java:422)<br>at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)<br>at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)<br> <br>at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)<br>at org.apache.hadoop.ipc.Client.call(Client.java:1457)<br>at org.apache.hadoop.ipc.Client.call(Client.java:1367)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)<br>at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)<br>at com.sun.proxy.$Proxy16.rollEditLog(Unknown Source)<br>at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.rollEditLog(NamenodeProtocolTranslatorPB.java:152)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:365)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$2.doWork(EditLogTailer.java:362)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:504)<br>at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>at java.lang.Thread.run(Thread.java:748)<br>2019-06-28 03:27:27,830 WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Unable to trigger a roll of the active NN<br>java.util.concurrent.ExecutionException: java.io.IOException: Cannot find any valid remote NN to service request!<br>at java.util.concurrent.FutureTask.report(FutureTask.java:122)<br>at java.util.concurrent.FutureTask.get(FutureTask.java:206)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer.triggerActiveLogRoll(EditLogTailer.java:380)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.doWork(EditLogTailer.java:430)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.access$300(EditLogTailer.java:399)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread$1.run(EditLogTailer.java:416)<br>at org.apache.hadoop.security.SecurityUtil.doAsLoginUserOrFatal(SecurityUtil.java:484)<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$EditLogTailerThread.run(EditLogTailer.java:412)<br>Caused by: java.io.IOException: Cannot find any valid remote NN to service request!<br>at org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer$MultipleNameNodeProxy.call(EditLogTailer.java:515)<br>at java.util.concurrent.FutureTask.run(FutureTask.java:266)<br>at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)<br>at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)<br>at java.lang.Thread.run(Thread.java:748)<br>2019-06-28 03:27:29,843 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1605ms<br>No GCs detected<br>2019-06-28 03:28:30,033 INFO org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Triggering log roll on remote NameNode<br>2019-06-28 03:28:30,278 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6d00c3eb expecting start txid #1<br>2019-06-28 03:28:30,279 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file http://dn3:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true, http://dn1:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true maxTxnsToRead = 9223372036854775807<br>2019-06-28 03:28:30,288 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream &#x27;http://dn3:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true, http://dn1:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true&#x27; to transaction ID 1<br>2019-06-28 03:28:30,288 INFO org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream: Fast-forwarding stream &#x27;http://dn3:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true&#x27; to transaction ID 1<br>2019-06-28 03:28:30,625 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file http://dn3:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true, http://dn1:8480/getJournal?jid=mycluster&amp;segmentTxId=1&amp;storageInfo=-65%3A1565240010%3A1561706674134%3ACID-d60aab16-2a77-48ed-8a5f-5699ca87b6bc&amp;inProgressOk=true of size 42 edits # 2 loaded in 0 seconds<br></code></pre></td></tr></table></figure><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>1、原因：两个 namenode 节点均处于 standby 状态，没有 active 状态的节点。</p><p>2、解决办法：</p><blockquote><p>集群挂掉时慎用<code>bin/hdfs haadmin -transitionToActive nn1</code>，尽量用<code>bin/hdfs haadmin -failover --forceactive nn1</code>：</p><p>These subcommands cause a given NameNode to transition to the Active or Standby state, respectively. These commands do not attempt to perform any fencing, and thus should rarely be used. Instead, one should almost always prefer to use the “hdfs haadmin -failover” subcommand</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">在想要转换为 active 状态的 namenode 的节点上操作</span><br>hdfs haadmin -transitionToActive --forcemanual nn1<br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;HDFS报错解决：Operation-category-JOURNAL-is-not-supported-in-state-sta</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    <category term="常见错误" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据常见错误：hdfs namenode -format 格式化报错找不到 JAVA_HOME</title>
    <link href="https://blog.yahyav2rayssr.top/posts/e05de508/"/>
    <id>https://blog.yahyav2rayssr.top/posts/e05de508/</id>
    <published>2023-04-16T09:02:40.843Z</published>
    <updated>2023-04-16T09:02:40.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="hdfs-namenode-format-格式化报错找不到-JAVA-HOME"><a href="#hdfs-namenode-format-格式化报错找不到-JAVA-HOME" class="headerlink" title="hdfs namenode -format 格式化报错找不到 JAVA_HOME"></a>hdfs namenode -format 格式化报错找不到 JAVA_HOME</h2><p>找到 etc&#x2F;hadoop 目录下的 hadoop-env.sh</p><p>增加如下配置：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">The java implementation to use. By default, this environment</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">variable is REQUIRED on ALL platforms except OS X!</span><br>export JAVA_HOME=/app/jdk1.8.0_212<br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;hdfs-namenode-format-格式化报错找不到-JAVA-HOME&quot;&gt;&lt;a href=&quot;#hdfs-namenode-</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    <category term="常见错误" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据常见错误：Hadoop启动成功，但50070端口无法访问</title>
    <link href="https://blog.yahyav2rayssr.top/posts/aa689eab/"/>
    <id>https://blog.yahyav2rayssr.top/posts/aa689eab/</id>
    <published>2023-04-16T09:02:30.656Z</published>
    <updated>2023-04-16T09:02:30.656Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="Hadoop启动成功，但50070端口无法访问"><a href="#Hadoop启动成功，但50070端口无法访问" class="headerlink" title="Hadoop启动成功，但50070端口无法访问"></a>Hadoop启动成功，但50070端口无法访问</h2><p>先查看自己的hadoop版本</p><p>我的 hadoop 是 3.2.3 版本的，hadoop-3.2.3 是我的 hadoop 目录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">/app/hadoop-3.2.3<br></code></pre></td></tr></table></figure><p>2.x.x 版本的 hadoop 默认端口为 <a href="http://ip:50070/">http://IP:50070</a></p><p>但是新出的 3.x.x 版本的默认端口为 <a href="http://ip:9870/">http://IP:9870</a></p><p>如果不是这个原因，那就去从机的 hadoop 的根目录下的 log 目录查看 log 文件，看看报的什么错误。再根据错误，查找相应的解决方法。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;Hadoop启动成功，但50070端口无法访问&quot;&gt;&lt;a href=&quot;#Hadoop启动成功，但50070端口无法访问&quot; class</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    <category term="常见错误" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据常见错误：bash v3.2+ is required. Sorry.</title>
    <link href="https://blog.yahyav2rayssr.top/posts/9170957e/"/>
    <id>https://blog.yahyav2rayssr.top/posts/9170957e/</id>
    <published>2023-04-16T09:02:18.988Z</published>
    <updated>2023-04-16T09:02:18.988Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="bash-v3-2-is-required-Sorry"><a href="#bash-v3-2-is-required-Sorry" class="headerlink" title="bash v3.2+ is required. Sorry."></a>bash v3.2+ is required. Sorry.</h2><p>Running with <em>root</em> user:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">start-dfs.sh</span> <br>Starting namenodes on [master]<br>bash v3.2+ is required. Sorry.<br>Starting datanodes<br>bash v3.2+ is required. Sorry.<br>Starting secondary namenodes [master_bis]<br>bash v3.2+ is required. Sorry<br></code></pre></td></tr></table></figure><p>Then I created a <em>hadoop</em> user and gave this user privileges on the Hadoop installation (R&#x2F;W access). After logging in with this new user I have the following output for the command that caused me some troubles:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash">start-dfs.sh</span> <br>Starting namenodes on [master]<br>Starting datanodes<br>Starting secondary namenodes [master_bis]<br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;bash-v3-2-is-required-Sorry&quot;&gt;&lt;a href=&quot;#bash-v3-2-is-required-Sorr</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    <category term="常见错误" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据常见错误：Attempting to operate on hdfs namenode as root</title>
    <link href="https://blog.yahyav2rayssr.top/posts/471a88e0/"/>
    <id>https://blog.yahyav2rayssr.top/posts/471a88e0/</id>
    <published>2023-04-16T09:02:10.651Z</published>
    <updated>2023-04-16T09:02:10.651Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="Attempting-to-operate-on-hdfs-namenode-as-root"><a href="#Attempting-to-operate-on-hdfs-namenode-as-root" class="headerlink" title="Attempting to operate on hdfs namenode as root"></a>Attempting to operate on hdfs namenode as root</h2><p>使用root配置的hadoop并启动会出现报错</p><p>错误：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs shell">Starting namenodes on [master]<br><br>ERROR: Attempting to operate on hdfs namenode as root<br><br>ERROR: but there is no HDFS_NAMENODE_USER defined. Aborting operation.<br><br>Starting datanodes<br><br>ERROR: Attempting to operate on hdfs datanode as root<br><br>ERROR: but there is no HDFS_DATANODE_USER defined. Aborting operation.<br><br>Starting secondary namenodes [slave1]<br><br>ERROR: Attempting to operate on hdfs secondarynamenode as root<br><br>ERROR: but there is no HDFS_SECONDARYNAMENODE_USER defined. Aborting operation.<br></code></pre></td></tr></table></figure><p>解决方法：</p><p>在 &#x2F;hadoop&#x2F;sbin 路径下：<br>将 start-dfs.sh，stop-dfs.sh 两个文件顶部添加以下参数</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs shell">HDFS_DATANODE_USER=root<br>HADOOP_SECURE_DN_USER=hdfs<br>HDFS_NAMENODE_USER=root<br>HDFS_SECONDARYNAMENODE_USER=root<br></code></pre></td></tr></table></figure><p>start-yarn.sh，stop-yarn.sh 顶部也需添加以下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">YARN_RESOURCEMANAGER_USER=root<br>HADOOP_SECURE_DN_USER=yarn<br>YARN_NODEMANAGER_USER=root<br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;Attempting-to-operate-on-hdfs-namenode-as-root&quot;&gt;&lt;a href=&quot;#Attempt</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    <category term="常见错误" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据常见错误：重新format namenode后，datanode无法正常启动</title>
    <link href="https://blog.yahyav2rayssr.top/posts/d2341114/"/>
    <id>https://blog.yahyav2rayssr.top/posts/d2341114/</id>
    <published>2023-04-16T09:02:00.147Z</published>
    <updated>2023-04-16T09:02:00.147Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="重新format-namenode后，datanode无法正常启动"><a href="#重新format-namenode后，datanode无法正常启动" class="headerlink" title="重新format namenode后，datanode无法正常启动"></a>重新format namenode后，datanode无法正常启动</h2><p>测试环境，由于测试需求，重新format namenode后，导致datanode无法正常启动。</p><p>查看datanode日志，可以发现错误“Initialization failed for Block pool <registering> (Datanode Uuid unassigned)”</registering></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs shell">2018-01-27 20:09:49,052 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool &lt;registering&gt; (Datanode Uuid unassigned) service to c6704/192.168.67.104:9000. Exiting.<br>java.io.IOException: All specified directories are failed to load.<br>at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:478)<br>at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)<br>at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)<br>at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:317)<br>at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)<br>at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)<br>at java.lang.Thread.run(Thread.java:745)<br>2018-01-27 20:09:49,056 FATAL org.apache.hadoop.hdfs.server.datanode.DataNode: Initialization failed for Block pool &lt;registering&gt; (Datanode Uuid unassigned) service to c6705/192.168.67.105:9000. Exiting.<br>java.io.IOException: All specified directories are failed to load.<br>at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:478)<br>at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1361)<br>at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1326)<br>at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:317)<br>at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:223)<br>at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:801)<br>at java.lang.Thread.run(Thread.java:745)<br>2018-01-27 20:09:49,069 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool &lt;registering&gt; (Datanode Uuid unassigned) service to c6705/192.168.67.105:9000<br>2018-01-27 20:09:49,070 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Ending block pool service for: Block pool &lt;registering&gt; (Datanode Uuid unassigned) service to c6704/192.168.67.104:9000<br>2018-01-27 20:09:49,192 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: Removed Block pool &lt;registering&gt; (Datanode Uuid unassigned)<br>2018-01-27 20:09:51,193 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Exiting Datanode<br>2018-01-27 20:09:51,204 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 0<br>2018-01-27 20:09:51,208 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG:<br>/************************************************************<br>SHUTDOWN_MSG: Shutting down DataNode at c6706.python279.org/192.168.67.106<br>************************************************************/<br></code></pre></td></tr></table></figure><p>经过百度，根据日志描述，原因是datanode的clusterID 和 namenode的clusterID 不匹配。打开hdfs-site.xml中关于datanode和namenode对应的目录，分别打开其中的current&#x2F;VERSION文件，进行对比。</p><p>namenode的VERSION内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[hdfs@c6704 $ cat /data/hadoop/hdfs/name/current/VERSION<br><span class="hljs-meta prompt_">#</span><span class="language-bash">Sat Jan 27 00:46:30 UTC 2018</span><br>namespaceID=1148548909<br>clusterID=CID-aedb2e82-77f2-4056-b676-dca88083215d<br>cTime=0<br>storageType=NAME_NODE<br>blockpoolID=BP-1099214307-192.168.67.104-1517013990445<br>layoutVersion=-63<br></code></pre></td></tr></table></figure><p>datanode的VERSION文件内容如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell">[hdfs@c6706 ~]$ cat /data/hadoop/hdfs/data/current/VERSION<br><span class="hljs-meta prompt_">#</span><span class="language-bash">Sat Jan 27 00:20:21 UTC 2018</span><br>storageID=DS-8f0fdd04-e967-43cd-bd41-93b826b675b8<br>clusterID=CID-b27ecfd8-64ba-4e43-bd82-4ef6f2edd60c<br>cTime=0<br>datanodeUuid=264b1b43-82c0-411c-859f-32761edc7465<br>storageType=DATA_NODE<br>layoutVersion=-56<br></code></pre></td></tr></table></figure><p>namenode和datanode的版本是不同的，决定备份datanode，并清空VERSION，然后启动datanode，问题依旧。检查VERSION，内容是空的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">[hdfs@c6706 current]$ cp VERSION VERSION.bk<br>[hdfs@c6706 current]$ echo &gt; VERSION<br>[hdfs@c6706 current]$ cat VERSION<br></code></pre></td></tr></table></figure><p>删除VERSION，再次启动datanode，VERSION内容已经同步。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">$ </span><span class="language-bash"><span class="hljs-built_in">cat</span> VERSION</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">Sun Jan 28 01:29:46 UTC 2018</span><br>storageID=DS-1c1f5e05-df2c-40de-b39b-d6d54e3c4894<br>clusterID=CID-aedb2e82-77f2-4056-b676-dca88083215d    ##&lt;&lt;&lt;&lt;&lt;同步了<br>cTime=0<br>datanodeUuid=948d5780-053e-4752-9476-fb1d1debda72<br>storageType=DATA_NODE<br>layoutVersion=-56<br></code></pre></td></tr></table></figure><p>通过页面也可以查询到datanode了。</p><p>问题原因分析</p><p>执行hdfs namenode -format后，current目录会删除并重新生成，其中VERSION文件中的clusterID也会随之变化，而datanode的VERSION文件中的clusterID保持不变，造成两个clusterID不一致。</p><p>所以为了避免这种情况，可以再执行的namenode格式化之后，删除datanode的current文件夹，或者修改datanode的VERSION文件中出clusterID与namenode的VERSION文件中的clusterID一样，然后重新启动datanode。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="http://blog.csdn.net/liuxinghao/article/details/40121843">http://blog.csdn.net/liuxinghao/article/details/40121843</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;重新format-namenode后，datanode无法正常启动&quot;&gt;&lt;a href=&quot;#重新format-namenode后，d</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    <category term="常见错误" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>zookeeper 安装</title>
    <link href="https://blog.yahyav2rayssr.top/posts/146a1eb6/"/>
    <id>https://blog.yahyav2rayssr.top/posts/146a1eb6/</id>
    <published>2023-04-16T09:00:56.545Z</published>
    <updated>2023-04-16T09:00:56.545Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><p>zookeeper 集群通常是用来对用户的分布式应用程序提供协调服务的，为了保证数据的一致性，对 zookeeper 集群进行了这样三种角色划分：leader、follower、observer 分别对应着总统、议员和观察者。</p><ul><li>总统（leader）：负责进行投票的发起和决议，更新系统状态。</li><li>议员（follower）：用于接收客户端请求并向客户端返回结果以及在选举过程中参与投票。</li><li>观察者（observer）：也可以接收客户端连接，将写请求转发给leader节点，但是不参与投票过程，只同步leader的状态。通常对查询操作做负载。</li></ul><h2 id="机器规划"><a href="#机器规划" class="headerlink" title="机器规划"></a>机器规划</h2><table><thead><tr><th>类型</th><th>IP地址</th><th>掩码</th><th>网关</th></tr></thead><tbody><tr><td>master</td><td>192.168.50.246</td><td>255.255.255.0</td><td>192.168.50.1</td></tr><tr><td>slave1</td><td>192.168.50.247</td><td>255.255.255.0</td><td>192.168.50.1</td></tr><tr><td>slave2</td><td>192.168.50.248</td><td>255.255.255.0</td><td>192.168.50.1</td></tr></tbody></table><h2 id="官网地址"><a href="#官网地址" class="headerlink" title="官网地址"></a>官网地址</h2><p><a href="https://zookeeper.apache.org/releases.html">https://zookeeper.apache.org/releases.html</a></p><h2 id="下载地址"><a href="#下载地址" class="headerlink" title="下载地址"></a>下载地址</h2><p><a href="https://archive.apache.org/dist/zookeeper/">https://archive.apache.org/dist/zookeeper/</a></p><h2 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h2><blockquote><p>所有节点都要安装。</p></blockquote><p>参考文章2</p><p>将zookeeper压缩文件解压后，我们进入到 conf 目录：</p><h2 id="文件配置"><a href="#文件配置" class="headerlink" title="文件配置"></a>文件配置</h2><h3 id="配置-zoo-cfg"><a href="#配置-zoo-cfg" class="headerlink" title="配置 zoo.cfg"></a>配置 zoo.cfg</h3><p>将zookeeper压缩文件解压后，我们进入到 conf 目录，将 zoo_sample.cfg 文件复制并重命名为 zoo.cfg 文件。</p><img src="/posts/146a1eb6/image-20221011233253265.png" alt="image-20221011233253265" style="zoom: 67%;"><p>配置文件修改如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">The number of milliseconds of each tick</span><br>tickTime=2000<br><span class="hljs-meta prompt_"># </span><span class="language-bash">The number of ticks that the initial</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">synchronization phase can take</span><br>initLimit=10<br><span class="hljs-meta prompt_"># </span><span class="language-bash">The number of ticks that can pass between</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">sending a request and getting an acknowledgement</span><br>syncLimit=5<br><span class="hljs-meta prompt_"># </span><span class="language-bash">the directory <span class="hljs-built_in">where</span> the snapshot is stored.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash"><span class="hljs-keyword">do</span> not use /tmp <span class="hljs-keyword">for</span> storage, /tmp here is just</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">example sakes.</span><br>dataDir=/usr/local/software/zookeeper-3.4.14/data<br><span class="hljs-meta prompt_"># </span><span class="language-bash">the port at <span class="hljs-built_in">which</span> the clients will connect</span><br>clientPort=2181<br>server.1=192.168.50.246:2888:3888<br>server.2=192.168.50.247:2888:3888<br>server.3=192.168.50.248:2888:3888<br><span class="hljs-meta prompt_"># </span><span class="language-bash">the maximum number of client connections.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">increase this <span class="hljs-keyword">if</span> you need to handle more clients</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">maxClientCnxns=60</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># Be sure to read the maintenance section of the</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">administrator guide before turning on autopurge.</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># The number of snapshots to retain in dataDir</span></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">autopurge.snapRetainCount=3</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">Purge task interval <span class="hljs-keyword">in</span> hours</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">Set to <span class="hljs-string">&quot;0&quot;</span> to <span class="hljs-built_in">disable</span> auto purge feature</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">autopurge.purgeInterval=1</span><br></code></pre></td></tr></table></figure><blockquote><p>参考官方文档：<a href="https://zookeeper.apache.org/doc/r3.5.8/zookeeperStarted.html">https://zookeeper.apache.org/doc/r3.5.8/zookeeperStarted.html</a></p></blockquote><p>主要是修改 dataDir 所对应的目录和增加server开头的三个节点的配置信息</p><ul><li><strong>tickTime</strong>：基本事件单元，这个时间是作为Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，每隔tickTime时间就会发送一个心跳；最小 的session过期时间为2倍tickTime</li><li><strong>dataDir</strong>：存储内存中数据库快照的位置，除非另有说明，否则指向数据库更新的事务日志。注意：应该谨慎的选择日志存放的位置，使用专用的日志存储设备能够大大提高系统的性能，如果将日志存储在比较繁忙的存储设备上，那么将会很大程度上影像系统性能。</li><li><strong>client</strong>：监听客户端连接的端口。</li><li><strong>initLimit</strong>：允许follower连接并同步到Leader的初始化连接时间，以tickTime为单位。当初始化连接时间超过该值，则表示连接失败。</li><li><strong>syncLimit</strong>：表示Leader与Follower之间发送消息时，请求和应答时间长度。如果follower在设置时间内不能与leader通信，那么此follower将会被丢弃。</li><li><strong>server.A&#x3D;B:C:D</strong></li></ul><p>　　　　A：其中 A 是一个数字，表示这个是服务器的编号；</p><p>　　　　B：是这个服务器的 ip 地址；</p><p>　　　　C：Zookeeper服务器之间的通信端口；</p><p>　　　　D：Leader选举的端口。</p><p>我们需要修改的第一个是 dataDir ,在指定的位置处创建好目录。</p><p>第二个需要新增的是 server.A&#x3D;B:C:D 配置，其中 A 对应下面我们即将介绍的myid 文件。B是集群的各个IP地址，C:D 是端口配置。</p><h3 id="创建-myid-文件"><a href="#创建-myid-文件" class="headerlink" title="创建 myid 文件"></a>创建 myid 文件</h3><p>在 上一步 dataDir 指定的目录下，创建 myid 文件。</p><img src="/posts/146a1eb6/image-20221011233910662.png" alt="image-20221011233910662" style="zoom:67%;"><p>server 节点配置信息：</p><figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs properties"><span class="hljs-attr">server.1</span>=<span class="hljs-string">192.168.50.246:2888:3888</span><br><span class="hljs-attr">server.2</span>=<span class="hljs-string">192.168.50.247:2888:3888</span><br><span class="hljs-attr">server.3</span>=<span class="hljs-string">192.168.50.248:2888:3888</span><br></code></pre></td></tr></table></figure><p>在 192.168.50.246 机器的的 &#x2F;usr&#x2F;local&#x2F;software&#x2F;zookeeper-3.3.6&#x2F;data 目录下创建 myid 文件，然后在该文件中写上 1 即可。</p><img src="/posts/146a1eb6/image-20221011234129145.png" alt="image-20221011234129145" style="zoom:67%;"><p>后面的机器依次在相应目录创建myid文件，写上相应配置数字即可。</p><h2 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h2><p>为了能够在任意目录启动zookeeper集群，我们需要配置环境变量。</p><p>ps:你也可以不配，这不是搭建集群的必要操作，只不过如果你不配置环境变量，那么每次启动zookeeper需要到安装文件的 bin 目录下去启动。</p><p>首先进入到 &#x2F;etc&#x2F;profile 目录，添加相应的配置信息：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">zookeeper</span><br>export ZK_HOME=/usr/local/software/zookeeper-3.4.14<br>export PATH=$PATH:$ZK_HOME/bin<br></code></pre></td></tr></table></figure><p>然后通过如下命令使得环境变量生效：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">source /etc/profle<br></code></pre></td></tr></table></figure><h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><p>启动命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">zkServer.sh start<br></code></pre></td></tr></table></figure><p>停止命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">zkServer.sh stop<br></code></pre></td></tr></table></figure><p>重启命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">zkServer.sh restart<br></code></pre></td></tr></table></figure><p>查看集群节点状态：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">zkServer.sh status<br></code></pre></td></tr></table></figure><p>我们分别对集群三台机器执行启动命令。执行完毕后，分别查看集群节点状态：</p><p>出现如下即是集群搭建成功：</p><img src="/posts/146a1eb6/image-20221011234618444.png" alt="image-20221011234618444" style="zoom:67%;"><img src="/posts/146a1eb6/image-20221011234647595.png" alt="image-20221011234647595" style="zoom:67%;"><img src="/posts/146a1eb6/image-20221011234722471.png" alt="image-20221011234722471" style="zoom:67%;"><p>三台机器， node2 成功的通过了选举称为了 leader，而剩下的两台成为了 follower。这时候，如果你将 node2 关掉，会发现剩下两台又会有一台变成了 leader 节点。</p><h2 id="集群测试"><a href="#集群测试" class="headerlink" title="集群测试"></a>集群测试</h2><p>集群搭建完毕，可以使用客户端连接任意一台服务器进行操作，连接服务器2，创建新的节点，连接服务器1，查看新创建的节点</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">[root@localhost opt]# zkCli.sh -server localhost:2181<br><br>[zk: localhost:2183(CONNECTED) 1] ls /<br>[zookeeper]<br>[zk: localhost:2183(CONNECTED) 2] create /mynode1 mydata1<br>Created /mynode1<br><br>[root@localhost opt]# zkCli.sh -server localhost:2181<br><br>[zk: localhost:2181(CONNECTED) 1] ls /<br>[mynode1, zookeeper]<br></code></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://www.cnblogs.com/ysocean/p/9860529.html">zookeeper 集群搭建</a></p><p><a href="https://cloud.tencent.com/developer/article/1820033">Zookeeper系列(4)：搭建Zookeeper集群</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;p&gt;zookeeper 集群通常是用来对用户的分布式应用程序提供协调服务的，为了保证数据的一致性，对 zookeeper 集群进行了这样三种角色</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="zookeeper" scheme="https://blog.yahyav2rayssr.top/tags/zookeeper/"/>
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>大数据日常运维命令</title>
    <link href="https://blog.yahyav2rayssr.top/posts/64ca3b01/"/>
    <id>https://blog.yahyav2rayssr.top/posts/64ca3b01/</id>
    <published>2023-04-16T09:00:07.395Z</published>
    <updated>2023-04-16T09:00:07.395Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="集群启动-x2F-停止方式总结"><a href="#集群启动-x2F-停止方式总结" class="headerlink" title="集群启动&#x2F;停止方式总结"></a>集群启动&#x2F;停止方式总结</h2><h3 id="单个"><a href="#单个" class="headerlink" title="单个"></a>单个</h3><h4 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h4><p>nameNode 启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start namenode<br></code></pre></td></tr></table></figure><p>nameNode 停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon stop namenode<br></code></pre></td></tr></table></figure><p>dataNode 启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start datanode<br></code></pre></td></tr></table></figure><p>dataNode 停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon stop datanode<br></code></pre></td></tr></table></figure><p>secondaryNamenode 启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon start secondarynamenode<br></code></pre></td></tr></table></figure><p>secondaryNamenode 停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs --daemon stop secondarynamenode<br></code></pre></td></tr></table></figure><h4 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h4><p>resourceManager 启动停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">yarn --daemon start resourcemanager<br>yarn --daemon stop resourcemanager<br></code></pre></td></tr></table></figure><p>nodeManager 启动停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">yarn --daemon start nodemanager<br>yarn --daemon stop nodemanager<br></code></pre></td></tr></table></figure><h3 id="批量（全机器已配置免密）"><a href="#批量（全机器已配置免密）" class="headerlink" title="批量（全机器已配置免密）"></a>批量（全机器已配置免密）</h3><h4 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h4><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./sbin/start-dfs.sh<br></code></pre></td></tr></table></figure><p>停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./sbin/stop-dfs.sh<br></code></pre></td></tr></table></figure><h4 id="YARN-1"><a href="#YARN-1" class="headerlink" title="YARN"></a>YARN</h4><p>启动</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./sbin/start-yarn.sh<br></code></pre></td></tr></table></figure><p>停止</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./sbin/stop-yarn.sh<br></code></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;集群启动-x2F-停止方式总结&quot;&gt;&lt;a href=&quot;#集群启动-x2F-停止方式总结&quot; class=&quot;headerlink&quot; ti</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title>SSH 免密登录配置</title>
    <link href="https://blog.yahyav2rayssr.top/posts/da9c3502/"/>
    <id>https://blog.yahyav2rayssr.top/posts/da9c3502/</id>
    <published>2023-04-16T08:59:42.364Z</published>
    <updated>2023-04-16T08:59:42.364Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="生成公钥和私钥"><a href="#生成公钥和私钥" class="headerlink" title="生成公钥和私钥"></a>生成公钥和私钥</h2><p>所有节点都要操作，命令如下：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-keygen -t rsa<br></code></pre></td></tr></table></figure><p>然后敲三个回车，就会生成两个文件 id_rsa（私钥）、id_rsa.pub（公钥），公钥里面的内容就是我们所需要的。</p><h2 id="将公钥拷贝到要免密登录的目标机器上"><a href="#将公钥拷贝到要免密登录的目标机器上" class="headerlink" title="将公钥拷贝到要免密登录的目标机器上"></a>将公钥拷贝到要免密登录的目标机器上</h2><p>所有节点都要操作</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-copy-id node1<br>ssh-copy-id node2<br>ssh-copy-id node3<br></code></pre></td></tr></table></figure><p>其实上面的命令相当于在 ~&#x2F;.ssh&#x2F;authorized_keys 下追加如下内容：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDOivJfUMuDNggjCIc1ZZrfbneSb7d8DaKTRPhTxwahGkwyhgB2WCmvtbAsGVqddYUOEKdCu7Nr87RwM7cMfoDCuzagaIZtRpk+amaJSwf65A4YF2Ss6D7U4Aj6i6UmKsmqtd9HHPKq0HAxA1Mvy/lp68O2HS1lsMOu9zfvf0i2pAnaGtVJwBUNlJ86sdSerl2NGNZtV5ZpIP3iVe5m2G5M6EftiKYN6z587nxACz8lqJ9yod8b2lD32fG6KN52r/olPI7ZQVMiCV3DHawT5TmJ2AhGM85MvuzY/2IkglxtMpvsrRiubC7QmOdyNnGfUjSfZLXdO0Us5KNBSZ6c7pkP hadoop@node1<br>...<br></code></pre></td></tr></table></figure><p>其中的内容刚好为 id_rsa.pub 公钥里面的内容，所以不通过 <code>ssh-copy-id</code> 的命令也是可行的，通过在当前用户下的 .ssh 目录创建 authorized_keys 文件并将所有节点的公钥内容补充到该文件下也能实现集群间两两彼此免密登录。</p><h2 id="集群分发脚本xsync"><a href="#集群分发脚本xsync" class="headerlink" title="* 集群分发脚本xsync"></a>* 集群分发脚本xsync</h2><p><strong>编辑文件</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /app<br>vim xsync<br></code></pre></td></tr></table></figure><p><strong>脚本内容</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_">#</span><span class="language-bash">!/bin/bash</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash">1. 判断参数个数</span><br>if [ $# -lt 1 ]<br>then<br>  echo Not Enough Arguement!<br>  exit;<br>fi<br><span class="hljs-meta prompt_">#</span><span class="language-bash">2. 遍历集群所有机器</span><br>for host in node1 node2 node3<br>do<br>  echo ====================  $host  ====================<br><span class="hljs-meta prompt_">  #</span><span class="language-bash">3. 遍历所有目录，挨个发送</span><br>  for file in $@<br>  do<br>    #4 判断文件是否存在<br>    if [ -e $file ]<br>    then<br>      #5. 获取父目录<br>      pdir=$(cd -P $(dirname $file); pwd)<br>      #6. 获取当前文件的名称<br>      fname=$(basename $file)<br>      ssh $host &quot;mkdir -p $pdir&quot;<br>      rsync -av $pdir/$fname $host:$pdir<br>    else<br>      echo $file does not exists!<br>    fi<br>  done<br>done<br></code></pre></td></tr></table></figure><p><strong>修改脚本 xsync 具有执行权限</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">chmod +x xsync<br></code></pre></td></tr></table></figure><p><strong>将脚本移动到 &#x2F;bin 中，以便全局调用</strong></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo mv xsync /bin/<br></code></pre></td></tr></table></figure><p><strong>测试脚本</strong></p><p>将当前机器的 &#x2F;bin&#x2F;xsync 文件 scp 到 node1 node2 node3 的相同目录下，相同目录覆盖传输。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo xsync /bin/xsync<br></code></pre></td></tr></table></figure><p>然后在其他机器输入 xsync 命令，发现命令均已生效，说明同步脚本已经执行成功。</p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://littlerpl.me/2019/11/08/ssh-interconnection/">多台linux服务器实现ssh免密互连</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;生成公钥和私钥&quot;&gt;&lt;a href=&quot;#生成公钥和私钥&quot; class=&quot;headerlink&quot; title=&quot;生成公钥和私钥&quot;&gt;&lt;/</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="SSH" scheme="https://blog.yahyav2rayssr.top/tags/SSH/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop 安装</title>
    <link href="https://blog.yahyav2rayssr.top/posts/dd1d33f7/"/>
    <id>https://blog.yahyav2rayssr.top/posts/dd1d33f7/</id>
    <published>2023-04-16T08:58:17.773Z</published>
    <updated>2023-04-16T08:58:17.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="安装包下载"><a href="#安装包下载" class="headerlink" title="安装包下载"></a>安装包下载</h2><p><a href="https://archive.apache.org/dist/hadoop/common/">https://archive.apache.org/dist/hadoop/common/</a></p><h2 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h2><p>上传hadoop安装包到 &#x2F;opt&#x2F;software&#x2F; 目录</p><p>解压到 &#x2F;app 目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">tar vf hadoop-3.2.3.tar.gz -C /app/<br></code></pre></td></tr></table></figure><h2 id="环境变量配置"><a href="#环境变量配置" class="headerlink" title="环境变量配置"></a>环境变量配置</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/profile<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">HADOOP_HOME</span><br>export HADOOP_HOME=/app/hadoop-3.2.3<br>export PATH=$PATH:$HADOOP_HOME/bin<br>export PATH=$PATH:$HADOOP_HOME/sbin<br></code></pre></td></tr></table></figure><p>执行source使环境变量生效</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo source /etc/profile<br></code></pre></td></tr></table></figure><h2 id="目录结构一览"><a href="#目录结构一览" class="headerlink" title="目录结构一览"></a>目录结构一览</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs shell">drwxr-xr-x. 2 hadoop hadoop    203 Mar 19  2022 bin<br>drwxr-xr-x. 4 hadoop hadoop     30 Oct  9 05:32 data<br>drwxr-xr-x. 3 hadoop hadoop     20 Mar 19  2022 etc<br>drwxr-xr-x. 2 hadoop hadoop    106 Mar 19  2022 include<br>drwxr-xr-x. 3 hadoop hadoop     20 Mar 19  2022 lib<br>drwxr-xr-x. 4 hadoop hadoop   4096 Mar 19  2022 libexec<br>-rw-rw-r--. 1 hadoop hadoop 150571 Mar  9  2022 LICENSE.txt<br>drwxr-xr-x. 3 hadoop hadoop   4096 Oct  9 08:47 logs<br>-rw-rw-r--. 1 hadoop hadoop  21943 Mar  9  2022 NOTICE.txt<br>-rw-rw-r--. 1 hadoop hadoop   1361 Mar  9  2022 README.txt<br>drwxr-xr-x. 3 hadoop hadoop   4096 Mar 19  2022 sbin<br>drwxr-xr-x. 4 hadoop hadoop     31 Mar 19  2022 share<br>drwxr-xr-x. 3 hadoop hadoop     40 Oct  9 06:01 tmp<br></code></pre></td></tr></table></figure><p>目录释义：</p><ul><li><strong>bin目录：</strong>存放对Hadoop相关服务（HDFS,YARN）进行操作的脚本</li><li><strong>etc目录：</strong>Hadoop的配置文件目录，存放Hadoop的配置文件</li><li><strong>lib目录：</strong>存放Hadoop的本地库（对数据进行压缩解压缩功能）</li><li><strong>sbin目录：</strong>存放启动或停止Hadoop相关服务的脚本</li><li><strong>share目录：</strong>存放Hadoop的依赖jar包、文档、和官方案例</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;安装包下载&quot;&gt;&lt;a href=&quot;#安装包下载&quot; class=&quot;headerlink&quot; title=&quot;安装包下载&quot;&gt;&lt;/a&gt;安装包下</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="Hadoop" scheme="https://blog.yahyav2rayssr.top/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>大数据安装部署（常规版）</title>
    <link href="https://blog.yahyav2rayssr.top/posts/f509a887/"/>
    <id>https://blog.yahyav2rayssr.top/posts/f509a887/</id>
    <published>2023-04-16T08:57:19.821Z</published>
    <updated>2023-04-16T08:57:19.821Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h1><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><blockquote><p>Hadoop运行模式包括：本地模式、伪分布式模式以及完全分布式模式。(这里使用完全分布式模式)。</p></blockquote><p>准备3台虚拟机，最低要求：内存4G，硬盘40G，这里准备的虚拟机是4G，硬盘40G的配置。</p><p>机器配置如下：</p><table><thead><tr><th>hostname</th><th>ip</th><th>内存</th><th>cpu</th><th>磁盘</th></tr></thead><tbody><tr><td>node1</td><td>192.168.50.246</td><td>4G</td><td>2c</td><td>40G</td></tr><tr><td>node2</td><td>192.168.50.247</td><td>4G</td><td>2c</td><td>40G</td></tr><tr><td>node3</td><td>192.168.50.248</td><td>4G</td><td>2c</td><td>40G</td></tr></tbody></table><h2 id="环境配置"><a href="#环境配置" class="headerlink" title="环境配置"></a>环境配置</h2><blockquote><p>所有节点都需要配置。</p></blockquote><h3 id="创建hadoop用户"><a href="#创建hadoop用户" class="headerlink" title="创建hadoop用户"></a>创建hadoop用户</h3><blockquote><p>后续操作都使用hadoop用户，不使用root用户进行操作。</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">useradd hadoop<br>passwd hadoop<br></code></pre></td></tr></table></figure><p>然后配置用户具有root权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /etc/sudoers<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"><span class="hljs-comment"># Allow root to run any commands anywhere</span></span><br>root  ALL=(ALL)     ALL<br>hadoop   ALL=(ALL)     ALL<br></code></pre></td></tr></table></figure><h3 id="机器时间同步"><a href="#机器时间同步" class="headerlink" title="机器时间同步"></a>机器时间同步</h3><p>安装依赖：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo yum install -y epel-release<br>sudo yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static<br></code></pre></td></tr></table></figure><p>时间同步（每台机器都要执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl start ntpd<br></code></pre></td></tr></table></figure><p>时间同步停止命令（不用执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">systemctl stop ntpd<br></code></pre></td></tr></table></figure><h3 id="修改主机名"><a href="#修改主机名" class="headerlink" title="修改主机名"></a>修改主机名</h3><p>每台机器设置各自的 hostname</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">node1</span><br>sudo hostnamectl --static set-hostname node1<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">node2</span><br>sudo hostnamectl --static set-hostname node2<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">node3</span><br>sudo hostnamectl --static set-hostname node3<br></code></pre></td></tr></table></figure><h3 id="设置host文件"><a href="#设置host文件" class="headerlink" title="设置host文件"></a>设置host文件</h3><p>所有机器都设置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo vim /etc/hosts<br><br>192.168.50.246   node1<br>192.168.50.247   node2<br>192.168.50.248   node3<br></code></pre></td></tr></table></figure><h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo systemctl stop firewalld<br>sudo systemctl disable firewalld<br></code></pre></td></tr></table></figure><h3 id="创建文件夹并授权给hadoop用户"><a href="#创建文件夹并授权给hadoop用户" class="headerlink" title="创建文件夹并授权给hadoop用户"></a>创建文件夹并授权给hadoop用户</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs shell">sudo mkdir /app<br>sudo chown -R hadoop:hadoop /app<br></code></pre></td></tr></table></figure><h3 id="SSH免密登录配置"><a href="#SSH免密登录配置" class="headerlink" title="SSH免密登录配置"></a>SSH免密登录配置</h3><p>参考文章5</p><h3 id="JDK安装"><a href="#JDK安装" class="headerlink" title="JDK安装"></a>JDK安装</h3><blockquote><p>所有节点都要安装。</p></blockquote><p>参考文章2</p><h3 id="Hadoop安装"><a href="#Hadoop安装" class="headerlink" title="Hadoop安装"></a>Hadoop安装</h3><blockquote><p>所有节点都要安装。</p></blockquote><p>参考文章3</p><h2 id="集群配置"><a href="#集群配置" class="headerlink" title="集群配置"></a>集群配置</h2><h3 id="机器规划"><a href="#机器规划" class="headerlink" title="机器规划"></a>机器规划</h3><table><thead><tr><th>服务</th><th>node1</th><th>node2</th><th>node3</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td>-</td><td>SecondaryNameNode</td></tr><tr><td>HDFS</td><td>DataNode</td><td>DateNode</td><td>DateNode</td></tr><tr><td>YARN</td><td>-</td><td>ResourceManager</td><td>-</td></tr><tr><td>YARN</td><td>NodeManager</td><td>NodeManager</td><td>NodeManager</td></tr></tbody></table><h3 id="WEB-端口信息"><a href="#WEB-端口信息" class="headerlink" title="WEB 端口信息"></a>WEB 端口信息</h3><table><thead><tr><th>服务</th><th>类型</th><th>访问地址</th></tr></thead><tbody><tr><td>HDFS</td><td>NameNode</td><td><a href="http://node1:50070/">http://node1:50070</a></td></tr><tr><td>HDFS</td><td>SecondaryNameNode</td><td><a href="http://node3:9868/">http://node3:9868</a></td></tr><tr><td>YARN</td><td>jobhistory</td><td><a href="http://node1:19888/">http://node1:19888</a></td></tr></tbody></table><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><blockquote><p>参考链接：<a href="https://blog.csdn.net/wjt199866/article/details/106473174">https://blog.csdn.net/wjt199866/article/details/106473174</a></p><p>更多配置参数信息，请参考官方网址查询</p><ul><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/core-default.xml</a></li><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml">http://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml</a></li><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml">http://hadoop.apache.org/docs/current/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml</a></li><li><a href="https://link.zhihu.com/?target=http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml">http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-common/yarn-default.xml</a></li></ul><p>通过这些网址，可以了解最新的全部的hadoop 配置信息，而且包括一些过时的定义标识，从而更好地维护您的集群。</p></blockquote><p>所有的配置文件都在 &#x2F;app&#x2F;hadoop-3.2.3&#x2F;etc&#x2F;hadoop 目录下，主要需要修改的配置文件如下：</p><h4 id="配置workers"><a href="#配置workers" class="headerlink" title="配置workers"></a>配置workers</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell">vim /app/hadoop-3.2.3/etc/hadoop/workers<br><br>node1<br>node2<br>node3<br></code></pre></td></tr></table></figure><h4 id="hadoop-env-sh"><a href="#hadoop-env-sh" class="headerlink" title="hadoop-env.sh"></a>hadoop-env.sh</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">The java implementation to use. By default, this environment</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">variable is REQUIRED on ALL platforms except OS X!</span><br>export JAVA_HOME=/app/jdk1.8.0_212<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># To prevent accidents, shell commands be (superficially) locked</span></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">to only allow certain <span class="hljs-built_in">users</span> to execute certain subcommands.</span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">It uses the format of (<span class="hljs-built_in">command</span>)_(subcommand)_USER.</span><br><span class="hljs-meta prompt_">#</span><span class="language-bash"></span><br><span class="language-bash"><span class="hljs-comment"># For example, to limit who can execute the namenode command,</span></span><br>export HDFS_NAMENODE_USER=&quot;hadoop&quot;<br>export HDFS_DATANODE_USER=&quot;hadoop&quot;<br>export HDFS_SECONDARYNAMENODE_USER=&quot;hadoop&quot;<br>export YARN_RESOURCEMANAGER_USER=&quot;hadoop&quot;<br>export YARN_NODEMANAGER_USER=&quot;hadoop&quot;<br>export HADOOP_PID_DIR=/app/hadoop-3.2.3/tmp/hadoop-hadoop-datanode.pid<br></code></pre></td></tr></table></figure><h4 id="core-site-xml"><a href="#core-site-xml" class="headerlink" title="core-site.xml"></a>core-site.xml</h4><blockquote><p>集群全局参数。</p><p>用于定义系统级别的参数，如 HDFS URL、Hadoop 的临时目录等。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 配置 hdfs 的地址，统一通信地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>fs.defaultFS<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hdfs://node1:8020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/app/hadoop-3.2.3/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 配置 hadoop 的临时目录 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.tmp.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>/hadoop/tmp<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 配置读写缓存大小 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>io.file.buffer.size<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>131072<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 代理用户配置 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.proxyuser.hadoop.hosts<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.proxyuser.hadoop.groups<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>*<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- hdfs界面设置操作文件 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>hadoop<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.permissions.enabled<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>false<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="hdfs-site-xml"><a href="#hdfs-site-xml" class="headerlink" title="hdfs-site.xml"></a>hdfs-site.xml</h4><blockquote><p>HDFS 参数。</p><p>如名称节点和数据节点的存放位置、文件副本的个数、文件读取权限等</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.name.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/name<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.datanode.data.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/data<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>file://$&#123;hadoop.data.dir&#125;/namesecondary<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.client.datanode-restart.timeout<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>30<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node3:9868<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>dfs.namenode.http-address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:50070<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="mapred-site-xml"><a href="#mapred-site-xml" class="headerlink" title="mapred-site.xml"></a>mapred-site.xml</h4><blockquote><p>Mapreduce 参数。</p><p>包括 JobHistory Server 和应用程序参数两部分，如 reduce 任务的默认个数、任务所能够使用内存的默认上下限等。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.framework.name<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>yarn<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 历史服务器端地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:10020<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 历史服务器web端地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node1:19888<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h4 id="yarn-site-xml"><a href="#yarn-site-xml" class="headerlink" title="yarn-site.xml"></a>yarn-site.xml</h4><blockquote><p>集群资源管理系统参数。</p><p>配置 ResourceManager，NodeManager 的通信端口，web监控端口等。</p></blockquote><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-meta">&lt;?xml version=<span class="hljs-string">&quot;1.0&quot;</span> encoding=<span class="hljs-string">&quot;UTF-8&quot;</span>?&gt;</span><br><span class="hljs-meta">&lt;?xml-stylesheet type=<span class="hljs-string">&quot;text/xsl&quot;</span> href=<span class="hljs-string">&quot;configuration.xsl&quot;</span>?&gt;</span><br><br><span class="hljs-tag">&lt;<span class="hljs-name">configuration</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>mapreduce_shuffle<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>node2<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-comment">&lt;!-- 日志采集 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>true<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log.server.url<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>http://node1:19888/jobhistory/logs<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">property</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="hljs-tag">&lt;/<span class="hljs-name">name</span>&gt;</span><br>        <span class="hljs-tag">&lt;<span class="hljs-name">value</span>&gt;</span>604800<span class="hljs-tag">&lt;/<span class="hljs-name">value</span>&gt;</span><br>    <span class="hljs-tag">&lt;/<span class="hljs-name">property</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">configuration</span>&gt;</span><br></code></pre></td></tr></table></figure><h3 id="分发配置文件"><a href="#分发配置文件" class="headerlink" title="分发配置文件"></a>分发配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">xsync /app/hadoop-3.2.3/etc/hadoop<br></code></pre></td></tr></table></figure><h3 id="HDFS-集群单点启动"><a href="#HDFS-集群单点启动" class="headerlink" title="HDFS 集群单点启动"></a>HDFS 集群单点启动</h3><p>如果集群是第一次启动，需要格式化 NameNode（node1 执行）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">./hdfs namenode -format<br></code></pre></td></tr></table></figure><p>在 node1 上启动 NameNode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./hdfs --daemon start namenode<br></code></pre></td></tr></table></figure><p><strong>完成后执行jps命令，查看进程。</strong></p><p>在 node1、node2、node3 上执行如下命令（三台都要执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">./hdfs --daemon start datanode<br></code></pre></td></tr></table></figure><h3 id="HDFS-集群批量启动（推荐）"><a href="#HDFS-集群批量启动（推荐）" class="headerlink" title="HDFS 集群批量启动（推荐）"></a>HDFS 集群批量启动（推荐）</h3><p>如果集群是第一次启动，需要在cdh01节点格式化NameNode（注意格式化之前，一定要先停止上次启动的所有 namenode 和 datanode 进程，然后再删除 data 和 log 数据），然后离谱执行如下命令：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">hdfs namenode -format<br></code></pre></td></tr></table></figure><p>删除 data 和 logs 数据</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /app/hadoop-3.2.3<br>rm -rf data/*<br>rm -rf logs/*<br></code></pre></td></tr></table></figure><p>node1 启动HDFS</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /app/hadoop-3.2.3<br><br>./sbin/start-dfs.sh<br></code></pre></td></tr></table></figure><h3 id="YARN-批量启动"><a href="#YARN-批量启动" class="headerlink" title="YARN 批量启动"></a>YARN 批量启动</h3><p>在配置了ResourceManager的节点（node2）启动YARN</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs shell">cd /app/hadoop-3.2.3<br><br>./sbin/start-yarn.sh<br></code></pre></td></tr></table></figure><h2 id="历史服务器配置"><a href="#历史服务器配置" class="headerlink" title="历史服务器配置"></a>历史服务器配置</h2><p>主要对应配置文件 mapred-site.xml，增加如下配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs shell">&lt;!-- 历史服务器端地址 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;<br>    &lt;value&gt;node1:10020&lt;/value&gt;<br>&lt;/property&gt;<br><br>&lt;!-- 历史服务器web端地址 --&gt;<br>&lt;property&gt;<br>    &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;<br>    &lt;value&gt;node1:19888&lt;/value&gt;<br>&lt;/property&gt;<br></code></pre></td></tr></table></figure><p>启动停止（node1 机器执行）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs shell"><span class="hljs-meta prompt_"># </span><span class="language-bash">启动</span><br>./bin/mapred --daemon start historyserver<br><span class="hljs-meta prompt_"></span><br><span class="hljs-meta prompt_"># </span><span class="language-bash">停止</span><br>./bin/mapred --daemon stop historyserver<br></code></pre></td></tr></table></figure><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p><a href="https://blog.csdn.net/sinat_37316828/article/details/112256427">Hadoop基础环境搭建完整版</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;目录&quot;&gt;&lt;a href=&quot;#目录&quot; class=&quot;headerlink&quot; title=&quot;目录&quot;&gt;&lt;/a&gt;目录&lt;/h1&gt;&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h</summary>
      
    
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
    <category term="安装部署" scheme="https://blog.yahyav2rayssr.top/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2/"/>
    
    
    <category term="大数据" scheme="https://blog.yahyav2rayssr.top/tags/%E5%A4%A7%E6%95%B0%E6%8D%AE/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://blog.yahyav2rayssr.top/posts/0/"/>
    <id>https://blog.yahyav2rayssr.top/posts/0/</id>
    <published>2023-04-16T08:53:40.217Z</published>
    <updated>2023-04-16T08:53:40.218Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、前奏"><a href="#一、前奏" class="headerlink" title="一、前奏"></a>一、前奏</h1><p>Hadoop是目前大数据领域最主流的一套技术体系，包含了多种技术。</p><p>包括HDFS（分布式文件系统），YARN（分布式资源调度系统），MapReduce（分布式计算系统），等等。</p><p>有些朋友可能听说过Hadoop，但是却不太清楚他到底是个什么东西，这篇文章就用大白话给各位阐述一下。</p><p>假如你现在公司里的数据都是放在MySQL里的，那么就全部放在一台数据库服务器上，我们就假设这台服务器的磁盘空间有2T吧，<strong>大家先看下面这张图。</strong></p><img src="/posts/0/1670dbfd11e62805tplv-t2oaga2asx-zoom-in-crop-mark3024000.webp" class alt="img"><p>现在问题来了，你不停的往这台服务器的MySQL里放数据，结果数据量越来越大了，超过了2T的大小了，现在咋办？</p><p>你说，我可以搞多台MySQL数据库服务器，分库分表啊！每台服务器放一部分数据不就得了。<strong>如上图所示！</strong></p><p>好，没问题，那咱们搞3台数据库服务器，3个MySQL实例，然后每台服务器都可以2T的数据。</p><p>现在我问你一个问题，<strong>所谓的大数据是在干什么？</strong></p><p>我们来说一下大数据最初级的一个使用场景。假设你有一个电商网站，现在要把这个电商网站里所有的用户在页面和APP上的点击、购买、浏览的行为日志都存放起来分析。</p><p>你现在把这些数据全都放在了3台MySQL服务器，数据量很大，但还是勉强可以放的下。</p><p>某天早上，你的boss来了。要看一张报表，比如要看每天网站的X指标、Y指标、Z指标，等等，二三十个数据指标。</p><p>好了，兄弟，现在你尝试去从那些点击、购买、浏览的日志里，通过写一个SQL来分析出那二三十个指标试试看？</p><p>我跟你打赌，你绝对会写出来一个几百行起步，甚至上千行的超级复杂大SQL。这个SQL，你觉得他能运行在分库分表后的3台MySQL服务器上么？</p><p>如果你觉得可以的话，那你一定是不太了解MySQL分库分表后有多坑，几百行的大SQL跨库join，各种复杂的计算，根本不现实。</p><p>所以说，大数据的存储和计算压根儿不是靠MySQL来搞的，因此，Hadoop、Spark等大数据技术体系才应运而生。</p><p>本质上，Hadoop、Spark等大数据技术，其实就是一系列的分布式系统。</p><p>比如hadoop中的HDFS，就是大数据技术体系中的核心基石，<strong>负责分布式存储数据，这是啥意思？别急，继续往下看。</strong></p><p>HDFS全称是Hadoop Distributed File System，是Hadoop的分布式文件系统。</p><p>它由很多机器组成，每台机器上运行一个DataNode进程，负责管理一部分数据。</p><p>然后有一台机器上运行了NameNode进程，NameNode大致可以认为是负责管理整个HDFS集群的这么一个进程，他里面存储了HDFS集群的所有元数据。</p><p>然后有很多台机器，每台机器存储一部分数据！好，HDFS现在可以很好的存储和管理大量的数据了。</p><p>这时候你肯定会有疑问：MySQL服务器也不是这样的吗？你要是这样想，那就大错特错了。</p><p>这个事情不是你想的那么简单的，HDFS天然就是分布式的技术，所以你上传大量数据，存储数据，管理数据，天然就可以用HDFS来做。</p><p>如果你硬要基于MySQL分库分表这个事儿，会痛苦很多倍，因为MySQL并不是设计为分布式系统架构的，他在分布式数据存储这块缺乏很多数据保障的机制。</p><p>好，你现在用HDFS分布式存储了数据，接着不就是要分布式来计算这些数据了吗？</p><p>对于分布式计算：</p><ul><li>很多公司用Hive写几百行的大SQL（底层基于MapReduce）</li><li>也有很多公司开始慢慢的用Spark写几百行的大SQL（底层是Spark Core引擎）。</li></ul><p>总之就是写一个大SQL，人家会拆分为很多的计算任务，放到各个机器上去，每个计算任务就负责计算一小部分数据，这就是所谓的分布式计算。</p><p>这个，绝对比你针对分库分表的MySQL来跑几百行大SQL要靠谱的多。</p><p>对于上述所说，老规矩，同样给大家来一张图，大伙儿跟着图来仔细捋一下整个过程。</p><img src="/posts/0/1670dc005dc982dctplv-t2oaga2asx-zoom-in-crop-mark3024000.webp" class alt="img">]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、前奏&quot;&gt;&lt;a href=&quot;#一、前奏&quot; class=&quot;headerlink&quot; title=&quot;一、前奏&quot;&gt;&lt;/a&gt;一、前奏&lt;/h1&gt;&lt;p&gt;Hadoop是目前大数据领域最主流的一套技术体系，包含了多种技术。&lt;/p&gt;
&lt;p&gt;包括HDFS（分布式文件系统），YARN（</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://blog.yahyav2rayssr.top/posts/0/"/>
    <id>https://blog.yahyav2rayssr.top/posts/0/</id>
    <published>2023-04-16T08:52:03.150Z</published>
    <updated>2023-04-16T08:52:03.150Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop组成部分"><a href="#Hadoop组成部分" class="headerlink" title="Hadoop组成部分"></a>Hadoop组成部分</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在上一篇文章我们主要讲了大数据的简单概念，什么是大数据，大数据的特点是什么？之后我们又从大数据扩展到<code>Hadoop</code>，讲了三个最主要的问题，<code>Hadoop</code>是什么，<code>Hadoop</code>发展史，<code>Hadoop</code>相较于其他大数据框架而言优势又是什么?</p><p>今天呢，我们依然沿着上一篇的脉络，去探索<code>Hadoop</code>的基本组成部分，是哪些技术有机地组合在了一起造就了<code>Hadoop</code>今天在大数据领域的出色表现，在<code>Hadoop2.0</code>之后，<code>Hadoop</code>主要由以下三个部分组成：</p><ul><li><code>Map - Reduce</code> ：负责计算</li><li><code>Yarn</code> ：负责资源调度</li><li><code>HDFS</code>: 负责数据的存储</li></ul><p>它们三个相辅相成，互相成就，当然本篇文章今天只是初略地带大家理解一下这三种技术在<code>Hadoop</code>中所起到的作用，具体更加详细的细节，在我们之后关于<code>Map-Reduce</code>和<code>HDFS</code>专题中会做更加详细的概述。</p><h2 id="Map-Ruduce-编程模型"><a href="#Map-Ruduce-编程模型" class="headerlink" title="Map-Ruduce 编程模型"></a>Map-Ruduce 编程模型</h2><p>首先平常看到这种英语概念，第一时间就是打开我们的谷歌翻译，Map的意思我想大家都知道，毕竟<code>java</code>中用的不能再多，<code>Reduce</code>是降低减少归纳的意思，所以Map-Reduce就是一个先分隔（map）再归纳（Reduce)的过程。</p><p>我们来看下定义：</p><p><code>MapReduce</code>是一个分布式运算程序的编程框架，是用户开发“基于<code>Hadoop</code>的数据分析应用”的核心框架。核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，<strong>并发运行</strong>在一个<code>Hadoop</code>集群上。</p><p><strong><code>MapReduce</code>主要可以概括为<code>map</code>阶段和<code>reduce</code>阶段。</strong></p><p>只看定义确实是有点晦涩，那Map-Reduce通俗理解是什么呢？还是我们上一篇文章讲的那个例子：</p><p>初中的时候，男生爱看玄幻小说，因为怕被教导主任查到，于是采用分布式存储的方案，把书分成几页几页的，放在不同的同学那边放着，但教导主任不是傻子，所谓道高一尺魔高一丈，最后还是被发现了，而是还放言今天要是不把这本书凑齐交到他办公室，全部都等着叫家长吧。</p><p>最后大家都把手里的残本交给了班长小明，小明根据页码排序整理好，交给了教导主任。</p><p>教导主任说你这不是闲的吗，天天不好好学习搁那看的这什么，<strong>头破苍穹</strong>，是英语书不好背了，还是数学书不好看了？这么着，你不是闲得慌吗，就这个<strong>萧炎</strong>，就他，你下去给我查查，整本书这个名字一共出现了多少次！不查完今天别想吃饭了！</p><p>小明想，这不是玩完了，我自己查，我得查到猴年马月才能查完。</p><blockquote><p>重点来了，传统的编程模型要是需要知道一本书中某个单词出现的频率，只能写个程序，遍历整个文件，如果几个字还好说，但是把斗破苍穹遍历一遍，需要的时间绝对够你吃顿饭的。</p><p>那不是还有多线程吗？</p><p>是有多线程，但是前提是我们得有一台多核或者多处理器的计算机，而且多线程的程序写起来也有点小复杂。</p></blockquote><p>但小明不傻啊，小明心想，mmp，又不是我一个人看的，为啥要我自己数，于是小明心生一计，回到班里，大家有福同享有难同当，老师现在让我数<strong>萧炎</strong>在书中一共出现了多少次，我自己数到明天也数不完，谁看的谁过来<strong>大家一人数几页</strong>，然后你们在下面数好了<strong>汇总</strong>一下交给我。</p><p>于是全班男生一人数了几十页，不到一个小时就数完了，小明成功渡过一劫。</p><p>这就是 Map - Reduce，我一个人算不过来了，我找十个人并行计算，最后把结果进行汇总，不用说也知道是什么思想了，数据结构中用的最多的<strong>分而治之</strong>。</p><p>当然，Map-Reduce肯定不止我们上面说的那么简单，具体实现细节还是略微有点繁琐的，详细的执行流程，原理到时候我们在Map-Reduce专题再细细分析。</p><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><p><code>Yarn</code>这个东西在<code>Hadoop2.x</code>时代才诞生，在遥远的<code>Hadoop1.x</code>时代，<code>Map-Reduce</code>不仅要负责<strong>计算</strong>，还要负责<strong>资源调度</strong>，简直是又当爹又当妈，一两天还好，时间长了<code>Map-Reduce</code>就受不了了，就向<code>Hadoop</code>总部提意见，总部肯定装作没听到啊，一个人干俩人的活儿不能再划算了。于是就不搭理<code>Map-Reduce</code>，后来有一天，<code>Map-Reduce</code>终于忍无可忍了，就甩袖子不干了，因为之前Map-Reduce又干计算又干资源调度，所以Map-Reduce甩袖子不干了，整个<code>Hadoop</code>计算和资源调度系统全都歇菜了。</p><p>耦合太严重，于是<code>Hadoop</code>觉得这不行，被Map-Reduce卡脖子可还得了？于是<code>Hadoop</code>又招了一个专门负责资源调度，就是<code>Yarn</code>，这样一来，Map-Reduce只负责计算，Yarn只负责资源调度，<code>Hadoop</code>内部瞬间和谐多了。再也没有出现过一人罢工，全员歇菜的问题了。</p><p>Yarn主要干四个事儿，分别是：</p><p><strong>ResourceManager（RM）：</strong></p><ul><li>处理客户端请求。</li><li>监控<code>NodeManager</code>。</li><li>启动或监控<code>ApplicationMaster</code>。</li><li>资源的分配与调度。</li></ul><p><strong>NodeManager（NM）：</strong></p><ul><li>管理单个节点上的资源。</li><li>处理来自<code>ResourceManager</code>的命令</li><li>处理来自<code>ApplicationMaster</code>的命令</li></ul><p><strong>ApplicationMaster（AM）：</strong></p><ul><li>负责数据的切分。</li><li>为应用程序申请资源并分配给内部的任务。</li><li>任务的监控与容错。</li></ul><p><strong>Container ：</strong></p><p> <code>Container</code>是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。</p><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p><code>HDFS </code>:<code> Hadoop</code>分布式文件系统（<code>Hadoop Distributed File System</code>),听名字就知道是<code>Hadoop</code>中负责文件存储部分的技术了。</p><p><code>HDFS</code>相对于前面的<code>Map-Reduce</code>和<code>Yarn</code>就比较容易理解了，<code>HDFS</code>架构主要分为三个部分：</p><p><strong>NameNode（nn）</strong>:</p><p>存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的<code>DataNode</code>等。</p><p><code>NameNode</code>主要存储文件的元数据，比如我们去图书馆借书，<code>NameNode</code>存的就是这个图书馆所有书籍的目录，作者，文件属性，以及书的位置等信息。</p><p><strong>DataNode(dn)</strong> ：</p><p><code>DataNode(dn)</code>：在本地文件系统存储文件块数据，以及块数据的校验和。</p><p>还是上面那个图书馆的例子，如果<code>NameNode</code>主要存的是目录的话，那么<code>DataNode</code>就是存书的书架，也就是我们实际的数据实际是在<code>DataNode</code>上存放的。</p><p><strong>Secondary NameNode(2nn)：</strong></p><p><code>Secondary NameNode(2nn)</code>：用来监控<code>HDFS</code>状态的辅助后台程序，每隔一段时间获取<code>HDFS</code>元数据的快照</p><p>看名字就知道了，和我们<code>Nginx</code>中讲的万一<code>Nginx</code>挂了是一个性质，你只有一个<code>NameNode</code>，万一不小心<code>NameNode</code>挂了，所有文件的元数据都没法儿访问，找不到文件的实际位置，那不就gg了吗，所以<code>Secondary NameNode(2nn)：</code>主要就起一个辅助备份的作用.</p><p>万一<code>NameNode</code>挂了，别怕，有<code>Secondary NameNode(2nn)</code>在，他那有备份，恢复都是小KS。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://github.com/hanshuaikang/HanShu-Note/blob/master/Hadoop%E5%88%9D%E7%BA%A7%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/%E5%86%99%E7%BB%99%E5%90%8E%E7%AB%AF%E7%9A%84Hadoop%E5%88%9D%E7%BA%A7%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B%EF%BC%9AHadoop%E7%BB%84%E6%88%90.md">写给后端的Hadoop初级入门教程：Hadoop组成部分。</a></p><p></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Hadoop组成部分&quot;&gt;&lt;a href=&quot;#Hadoop组成部分&quot; class=&quot;headerlink&quot; title=&quot;Hadoop组成部分&quot;&gt;&lt;/a&gt;Hadoop组成部分&lt;/h1&gt;&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerli</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://blog.yahyav2rayssr.top/posts/0/"/>
    <id>https://blog.yahyav2rayssr.top/posts/0/</id>
    <published>2023-04-16T08:52:03.150Z</published>
    <updated>2023-04-16T08:52:03.150Z</updated>
    
    <content type="html"><![CDATA[<h1 id="概念篇"><a href="#概念篇" class="headerlink" title="概念篇"></a>概念篇</h1><h2 id="什么是大数据"><a href="#什么是大数据" class="headerlink" title="什么是大数据"></a>什么是大数据</h2><p>大数据 (Big Data) : 主要是指<code>无法在一定范围</code>内用常规软件工具进行捕捉，管理和处理的数据集合，是需要新处理模式才能具有更强的决策力，洞察发现力和流程优化能力的<code>海量，高增长率和多样化的信息资产</code>。</p><p>一句话解释：<strong>大数据就是大量数据，数据多到传统方案无法处理的程度。</strong></p><p>当然数据的体量并不是最重要的，重要的是隐藏在这些数据中的<code>信息</code>，这些信息不论是在商业上还是在研究上都有着巨大的价值，电商通过挖掘这些数据中的信息为每个用户画像，并且推荐合适的商品给用户增加购买，当然，也可以顺便调整一下改个价格杀个熟什么。</p><h3 id="大数据的单位"><a href="#大数据的单位" class="headerlink" title="大数据的单位"></a>大数据的单位</h3><p>但我们毕竟是严谨的理科生啊，你说大数据大数据，多大才是大数据？为了解决这个问题，减少撕逼，科学家就制定了一系列的数据单位，从小到大依次是：</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs autohotkey">bit` `Byte` `KB` `MB` `GB` `TB` `PB` `EB` `ZB` `YB` `BB` `NB(牛逼)` 和 `DB（呆逼）<br></code></pre></td></tr></table></figure><p>当然，光讲这些单位有什么意思，我怎么能知道这些单位能存多少数据？为了方便大家更加直接的感受到这些数据单位的威力，我找了一些小栗子：</p><ul><li>全世界所产生的印刷材料的数据大概是200PB。</li><li>全世界人类总共说过的话大概是5EB。</li><li>国外知名网站P站2017年网站产生的总数据量为3732PB 。</li><li>一百万个汉字大概所需要的内存是2MB。</li></ul><p>刚才好像混入了什么奇怪的东西。</p><h3 id="大数据的特点"><a href="#大数据的特点" class="headerlink" title="大数据的特点"></a>大数据的特点</h3><ul><li><code>大量：</code>必须的，不大都不好意思叫大数据。</li><li><code>高速：</code>这么多数据肯定要快速消化掉的，处理几十年也等不起啊，今年双十一的成交额总不能算到明年双十一再公布吧。</li><li><code>多样：</code>不同的场景会产生不同的数据，优酷就是用户浏览数据，视频数据，QQ音乐就是音乐数据。</li><li><code>低价值密度：</code>这个意思是<strong>即使数据量很大，但是我们关注的始终的特定的部分，而非整体</strong>，就像警察叔叔调监控一样，一年前一个月前的数据通常对他来说是没什么用的，他只要那么几个关键节点的监控数据就可以了。</li></ul><p>应用场景就不说了，哪都是应用场景。</p><h2 id="Hadoop是什么？"><a href="#Hadoop是什么？" class="headerlink" title="Hadoop是什么？"></a>Hadoop是什么？</h2><p>知道了什么是大数据，我们就得思考另外一个问题，弄这么多的数据我放哪啊？</p><p><code>杠精：</code>不明摆着的么，当然放硬盘里啊，要不放哪儿，还能写纸上？ <code>我：</code>硬盘我知道，可是万一这块硬盘坏了，那数据不就没了吗？</p><p><code>路人</code>：你系不系傻，你多放几块硬盘，分别放上去不就行了吗？</p><p>这个时候<code>Hadoop</code>来了，弟弟们都往边上靠靠，你们那种办法太笨拙，交给我，轻轻松松地给你搞定，小意思。</p><p><code>Hadoop</code>是一个由Apache基金会所开发的分布式系统基础架构，主要用来解决大数据的存储和分析计算问题。</p><p>当然，<code>Hadoop</code>和<code>Spring</code>一样，到现在已经没法去仅仅理解为<code>Hadoop</code>这门技术了，就像你跟别人说，我这个新电商项目基于<code>Spring</code>写的，那别人肯定不会觉得你只用了<code>Spring</code>，会觉得你可能用了<code>Spring MVC</code>，<code>boot</code>，<code>JPA</code>等一系列<code>Spring</code>生态的技术。同样地，<code>Hadoop</code>也是如此，不仅仅是代表<code>Hadoop</code>本身这项技术，同时也代表围绕<code>Hadoop</code>的技术生态。</p><p>而且大家千万不要把事情想复杂，以为分布式存储什么这些概念都是多么深奥的东西，的确，官方概念确实是有点抽象晦涩了，但是我觉得，<strong>任何一项理论都一定来源于生活，因为是生活给予了他们灵感，但是生活并不是十分复杂的，所以任何深奥复杂的理论一定可以在生活中找到一个通俗易懂的解释。</strong></p><p>什么是分布式存储，不跟大家吹，我初中的时候就已经在搞这个了，那时候流行看玄幻小说，那种大部头知道吧，特厚，通常一个班就只有那么一本，被教导主任没收了就完蛋了，谁都没得看，于是当时盛行把一本玄幻小说一页一页撕下来，每个同学几页，大家互相换着看，就算老师发现了也就只是没收了一部分，没办法全部歼灭。你看，分布式有了，存储有了，这不就是分布式存储吗？为了防止一本书被老师没收了导致这本书不完整，那就买三本，也这么几页几页分开存，这不就是多备份吗，没那么复杂，别老纠结那些学者写的给学者看的概念。</p><h2 id="Hadoop发展史"><a href="#Hadoop发展史" class="headerlink" title="Hadoop发展史"></a>Hadoop发展史</h2><p>这个也没啥好讲的，我这里就列几个关键的点，感兴趣的朋友下去可以自己搜，网上一搜一大堆。</p><ul><li>一个叫Dung Cutting 没事用java写了一个全文搜索的框架 - <code>Lucene</code></li><li>数据量大的时候，<code>Lucene</code>性能跟不上了就。</li><li>巧了，Google本身也是做全文搜索的，为啥人家性能就那么顶呢？</li><li>通过学习谷歌，搞了个<code>Nutch</code></li><li>后来谷歌公开了部分<code>GFS</code>和<code>MapReduce</code>的细节。</li><li>Dung Cutting 一看这答案都给自己了，于是花了两年，注意是业余时间，自己实现了<code>DFS</code>和<code>MapReduce</code>，<code>Nutch</code>性能一下字就提上去了，一个字，牛逼。</li><li>后来<code>Hadoop</code>作为<code>Lucene</code>子项目<code>Nutch</code>的一部分被正式引进了Apache基金会。</li><li>然后<code>Map-Reduce</code>和<code>NDFS</code>一块被整合进了<code>Hadoop</code>项目里面，<code>Hadoop</code>就这么诞生了。</li></ul><p>为啥人家业余时间就能搞出来这么牛逼的东西，我业余时间王者荣耀王者都上不去，难道有中间商赚差价？</p><h2 id="Hadoop发行版本"><a href="#Hadoop发行版本" class="headerlink" title="Hadoop发行版本"></a>Hadoop发行版本</h2><p>和Linux差不多，不同的公司在此基础上分别定制了自己的发行版本，<code>Hadoop</code>发行版本主要有三个，分别是：</p><ul><li>Apache版本：最原始（最基础）的版本，对于入门学习最好，毕竟是出生地，血统也是最正的。</li><li>Cloudera：在大型互联网企业中用的较多。</li><li>Hortonworks：文档比较全。</li></ul><p>不用想，我们肯定选Apache，也没啥别的原因，就是因为它基础，简单，不要钱。</p><h2 id="Hadoop优势是什么？"><a href="#Hadoop优势是什么？" class="headerlink" title="Hadoop优势是什么？"></a>Hadoop优势是什么？</h2><p><code>Hadoop</code>为啥这么牛逼，导致我们现在一说大数据开发，就会想到Hadoop？</p><p>毕竟写程序不是谈恋爱，没什么就算你不好我也依然爱你这回事，我们坏得很，哪个好用使哪个。</p><p><code>Hadoop</code>在江湖中能混到今天的地位主要靠以下四点：</p><ul><li><strong>高可靠性</strong>：<code>Hadoop</code>底层使用多个数据副本，即使<code>Hadoop</code>某个计算元素或存储出现故障，也不会导致数据的丢失，想想上面讲的分布式存储的例子。</li><li><strong>高扩展性</strong>：在集群间分配任务数据，可以方便的扩展数以千计的节点。就是，有一天运维早上一上班，卧槽，集群存储不够了，但是问题不大，因为在集群中加入一个新的节点或者去掉一个节点都分分钟的事儿。</li><li><strong>高效性</strong>：在<code>MapReduce</code>的思想下，<code>Hadoop</code>是并行工作的，以加快任务处理速度。</li><li><strong>高容错性</strong>：能够将失败的任务重新分配。</li></ul><p>你说了一堆优点，<code>Hadoop</code>就没啥缺点吗？必须有，但是这个要到后面写到<code>HDFS</code>，<code>MR</code>的时候才能说，要不现在都不知道<code>Hdfs</code>是啥，说缺点的话不形象，<strong>就跟说人坏话一样，当着人家面儿说才有效果。</strong></p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><p><a href="https://juejin.cn/post/6844904016174907405#heading-7">写给后端的Hadoop初级入门教程：概念篇</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;概念篇&quot;&gt;&lt;a href=&quot;#概念篇&quot; class=&quot;headerlink&quot; title=&quot;概念篇&quot;&gt;&lt;/a&gt;概念篇&lt;/h1&gt;&lt;h2 id=&quot;什么是大数据&quot;&gt;&lt;a href=&quot;#什么是大数据&quot; class=&quot;headerlink&quot; title=&quot;什么是大数据&quot;&gt;&lt;</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title></title>
    <link href="https://blog.yahyav2rayssr.top/posts/0/"/>
    <id>https://blog.yahyav2rayssr.top/posts/0/</id>
    <published>2023-04-16T08:52:03.150Z</published>
    <updated>2023-04-16T08:52:03.150Z</updated>
    
    <content type="html"><![CDATA[<h1 id="一、命令行登录方式"><a href="#一、命令行登录方式" class="headerlink" title="一、命令行登录方式"></a>一、命令行登录方式</h1><p>进入 hbase bin 目录下，执行以下命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">hbase shell<br></code></pre></td></tr></table></figure><h1 id="二、命名空间"><a href="#二、命名空间" class="headerlink" title="二、命名空间"></a>二、命名空间</h1><h2 id="1-查看所有命名空间"><a href="#1-查看所有命名空间" class="headerlink" title="1. 查看所有命名空间"></a>1. 查看所有命名空间</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">list_namespace<br></code></pre></td></tr></table></figure><h2 id="2-查看具体的命名空间"><a href="#2-查看具体的命名空间" class="headerlink" title="2. 查看具体的命名空间"></a>2. 查看具体的命名空间</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">describe_namespace <span class="hljs-string">&#x27;&lt;namespace&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="3-查看命名空间下的所有表"><a href="#3-查看命名空间下的所有表" class="headerlink" title="3. 查看命名空间下的所有表"></a>3. 查看命名空间下的所有表</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">list_namespace_tables <span class="hljs-string">&#x27;&lt;namespace&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h1 id="三、表操作"><a href="#三、表操作" class="headerlink" title="三、表操作"></a>三、表操作</h1><h2 id="1-查看所有表"><a href="#1-查看所有表" class="headerlink" title="1. 查看所有表"></a>1. 查看所有表</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">list<br></code></pre></td></tr></table></figure><h2 id="2-查看表结构"><a href="#2-查看表结构" class="headerlink" title="2. 查看表结构"></a>2. 查看表结构</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs shell">describe &#x27;&lt;table name&gt;&#x27;<br></code></pre></td></tr></table></figure><h2 id="3-扫描表（scan）"><a href="#3-扫描表（scan）" class="headerlink" title="3. 扫描表（scan）"></a>3. 扫描表（scan）</h2><blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, &#123; options &#125;<br></code></pre></td></tr></table></figure><p>HBase支持以下几种附加方式：</p><ol><li><code>COLUMNS</code>：列过滤</li><li><code>LIMIT</code>：限制查询结果行数</li><li><code>STARTROW</code>：<code>ROWKEY</code>起始行。会先根据这个<code>key</code>定位到<code>region</code>，再向后扫描</li><li><code>STOPROW</code>：结束行</li><li><code>TIMERANGE</code>：限定时间戳范围</li><li><code>VERSIONS</code>：版本数</li><li><code>FILTER</code>：按条件过滤行</li></ol></blockquote><h3 id="1-扫描所有数据"><a href="#1-扫描所有数据" class="headerlink" title="1. 扫描所有数据"></a>1. 扫描所有数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;table_name&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-扫描限制数量-2"><a href="#2-扫描限制数量-2" class="headerlink" title="2. 扫描限制数量 2"></a>2. 扫描限制数量 2</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table_name&gt;&#x27;</span>, &#123; LIMIT=&gt;2 &#125;<br>scan <span class="hljs-string">&#x27;&lt;table_name&gt;&#x27;</span>, &#123; FILTER=&gt;<span class="hljs-string">&quot;PageFilter(2)&quot;</span> &#125;<br></code></pre></td></tr></table></figure><h3 id="3-设置扫描起点终点（根据-rowkey）"><a href="#3-设置扫描起点终点（根据-rowkey）" class="headerlink" title="3. 设置扫描起点终点（根据 rowkey）"></a>3. 设置扫描起点终点（根据 rowkey）</h3><blockquote><p>STARTROW、ENDROW 等需要大写</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;table_name&#x27;</span>, &#123; STARTROW =&gt; <span class="hljs-string">&#x27;row10&#x27;</span>, ENDROW =&gt; <span class="hljs-string">&#x27;row20&#x27;</span> &#125;<br></code></pre></td></tr></table></figure><h3 id="4-扫描整个列簇"><a href="#4-扫描整个列簇" class="headerlink" title="4. 扫描整个列簇"></a>4. 扫描整个列簇</h3><blockquote><p>列簇名（column family）中间可以有空格。</p></blockquote><p><strong>语法：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, &#123;COLUMN=&gt;<span class="hljs-string">&#x27;&lt;column family:key&gt;&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><p><strong>举个栗子：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;dds_eas_AJ:oms_origin_waybill&#x27;</span>, &#123;COLUMN=&gt;<span class="hljs-string">&#x27;f:waybillNo&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><h3 id="5-根据时间戳扫描"><a href="#5-根据时间戳扫描" class="headerlink" title="5. 根据时间戳扫描"></a>5. 根据时间戳扫描</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">TIMERANGE<br></code></pre></td></tr></table></figure><h3 id="6-添加过滤"><a href="#6-添加过滤" class="headerlink" title="6. 添加过滤"></a>6. 添加过滤</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">FITLER<br></code></pre></td></tr></table></figure><h2 id="4-获取行或单元数据（get）"><a href="#4-获取行或单元数据（get）" class="headerlink" title="4. 获取行或单元数据（get）"></a>4. 获取行或单元数据（get）</h2><blockquote><p>rowkey 为 row_index。</p></blockquote><h3 id="1-获取指定行"><a href="#1-获取指定行" class="headerlink" title="1. 获取指定行"></a>1. 获取指定行</h3><p><strong>语法：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">get <span class="hljs-string">&#x27;&lt;namespace&gt;:&lt;table_name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;row_index&gt;&#x27;</span><br></code></pre></td></tr></table></figure><p><strong>举个栗子：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">get <span class="hljs-string">&#x27;dds_eas_AJ:oms_origin_waybill&#x27;</span>, <span class="hljs-string">&#x27;00AJ15e4571798369&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-获取指定行的指定列簇中列的数据"><a href="#2-获取指定行的指定列簇中列的数据" class="headerlink" title="2. 获取指定行的指定列簇中列的数据"></a>2. 获取指定行的指定列簇中列的数据</h3><p><strong>语法：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">get <span class="hljs-string">&#x27;&lt;namespace&gt;:&lt;table_name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;row_index&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;column_cluster&gt;:&lt;column_name&gt;&#x27;</span><br>get <span class="hljs-string">&#x27;&lt;namespace&gt;:&lt;table_name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;row_index&gt;&#x27;</span>, &#123;COLUMN=&gt;<span class="hljs-string">&#x27;&lt;column_cluster&gt;:&lt;column_name&gt;&#x27;</span>, TIMESTAMP=&gt;&lt;TIMESTAMP&gt;&#125;<br></code></pre></td></tr></table></figure><p><strong>举个栗子：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">get <span class="hljs-string">&#x27;dds_eas_AJ:oms_origin_waybill&#x27;</span>, <span class="hljs-string">&#x27;00AJ1504571798369&#x27;</span>, <span class="hljs-string">&#x27;f:waybillNo&#x27;</span><br>get <span class="hljs-string">&#x27;dds_eas_AJ:oms_origin_waybill&#x27;</span>, <span class="hljs-string">&#x27;00AJ1504571798369&#x27;</span>, &#123;COLUMN=&gt;<span class="hljs-string">&#x27;f:waybillNo&#x27;</span>&#125;<br></code></pre></td></tr></table></figure><h2 id="6-表是否存在"><a href="#6-表是否存在" class="headerlink" title="6. 表是否存在"></a>6. 表是否存在</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">exists <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="7-表是否-enable"><a href="#7-表是否-enable" class="headerlink" title="7. 表是否 enable"></a>7. 表是否 enable</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">is_enabled <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br>is_disabled <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br><br><span class="hljs-comment"># 如果要启动/禁用表，使用以下命令：</span><br>enabled <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br>disabled <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="8-统计表数据行数"><a href="#8-统计表数据行数" class="headerlink" title="8. 统计表数据行数"></a>8. 统计表数据行数</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">count <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="9-删除（delete，truncate）"><a href="#9-删除（delete，truncate）" class="headerlink" title="9. 删除（delete，truncate）"></a>9. 删除（delete，truncate）</h2><h3 id="1-清空表"><a href="#1-清空表" class="headerlink" title="1. 清空表"></a>1. 清空表</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">truncate</span> <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="2-删除表"><a href="#2-删除表" class="headerlink" title="2. 删除表"></a>2. 删除表</h3><p>先要屏蔽该表，才能对该表进行删除，步骤如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">disable</span> <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br>drop <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="3-删除行数据"><a href="#3-删除行数据" class="headerlink" title="3. 删除行数据"></a>3. 删除行数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">deleteall <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;row&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h3 id="4-删除列数据"><a href="#4-删除列数据" class="headerlink" title="4. 删除列数据"></a>4. 删除列数据</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">delete <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;row&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;column name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;time stamp&gt;&#x27;</span><br></code></pre></td></tr></table></figure><h2 id="10-过滤-FILTER"><a href="#10-过滤-FILTER" class="headerlink" title="10. 过滤 FILTER"></a>10. 过滤 FILTER</h2><blockquote><p>FILTER中支持多个过滤条件通过括号、AND和OR的条件组合</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;member&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;ColumnPrefixFilter(&#x27;birth&#x27;) AND ValueFilter(=,&#x27;substring:1988&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><p>查看 shell 中定义了哪些 filter 常量，如果想要使用 shell 中未定义的常量，在使用的时候必须手动 import filter 的全路径。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">show_filters<br></code></pre></td></tr></table></figure></blockquote><h3 id="比较器"><a href="#比较器" class="headerlink" title="比较器"></a>比较器</h3><ol><li><strong>二进制比较器：</strong>如’binary:abc’，按字典排序跟’abc’进行比较</li><li><strong>二进制前缀比较器：</strong>如’binaryprefix:abc’，按字典顺序只跟’abc’比较前3个字符</li><li><strong>正则表达式比较器（重要）：</strong>如’regexstring:ab*yz’，按正则表达式匹配以ab开头，以yz结尾的值。这个比较器只能使用&#x3D;、!&#x3D;两个比较运算符。</li><li><strong>子串比较器：</strong>如’substring:abc123’，匹配以abc123开头的值。这个比较顺也只能使用&#x3D;、!&#x3D;两个比较运算符。</li></ol><h3 id="列过滤"><a href="#列过滤" class="headerlink" title="列过滤"></a>列过滤</h3><h4 id="1-限制某个列的值等于26"><a href="#1-限制某个列的值等于26" class="headerlink" title="1. 限制某个列的值等于26"></a>1. 限制某个列的值等于26</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;ValueFilter(=,&#x27;binary:26&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h4 id="2-值包含6这个值"><a href="#2-值包含6这个值" class="headerlink" title="2. 值包含6这个值"></a>2. 值包含6这个值</h4><blockquote><p>注：substring不能使用小于等于等符号</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;ValueFilter(=,&#x27;substring:6&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h4 id="3-列名中的前缀为E的"><a href="#3-列名中的前缀为E的" class="headerlink" title="3. 列名中的前缀为E的"></a>3. 列名中的前缀为E的</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;ColumnPrefixFilter(&#x27;E&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h4 id="4-列簇含f的数据"><a href="#4-列簇含f的数据" class="headerlink" title="4. 列簇含f的数据"></a>4. 列簇含f的数据</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;FamilyFilter(=,&#x27;substring:f&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h3 id="行过滤"><a href="#行过滤" class="headerlink" title="行过滤"></a>行过滤</h3><blockquote><p><code>PrefixFilter</code>是对 Rowkey 的前缀进行判断，这是一个非常常用的功能。</p></blockquote><h4 id="1-限制某行的值等于26"><a href="#1-限制某行的值等于26" class="headerlink" title="1. 限制某行的值等于26"></a>1. 限制某行的值等于26</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;RowFilter(=,&#x27;binary:26&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h4 id="2-值包含6这个值-1"><a href="#2-值包含6这个值-1" class="headerlink" title="2. 值包含6这个值"></a>2. 值包含6这个值</h4><blockquote><p>注：substring不能使用小于等于等符号</p></blockquote><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;RowFilter(=,&#x27;substring:6&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h4 id="3-行名中的前缀为E的"><a href="#3-行名中的前缀为E的" class="headerlink" title="3. 行名中的前缀为E的"></a>3. 行名中的前缀为E的</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">scan <span class="hljs-string">&#x27;&lt;table name&gt;&#x27;</span>, FILTER=&gt;<span class="hljs-string">&quot;PrefixFilter(&#x27;E&#x27;)&quot;</span><br></code></pre></td></tr></table></figure><h2 id="11-插入数据（put）"><a href="#11-插入数据（put）" class="headerlink" title="11. 插入数据（put）"></a>11. 插入数据（put）</h2><p><strong>语法：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">put <span class="hljs-string">&#x27;&lt;table_name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;row_id&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;column_family&gt;:&lt;column_name&gt;&#x27;</span>, <span class="hljs-string">&#x27;&lt;value&gt;&#x27;</span><br></code></pre></td></tr></table></figure><p><strong>举个栗子：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">put <span class="hljs-string">&#x27;member&#x27;</span>, <span class="hljs-string">&#x27;debugo&#x27;</span>,<span class="hljs-string">&#x27;info:age&#x27;</span>, <span class="hljs-string">&#x27;27&#x27;</span><br></code></pre></td></tr></table></figure><h1 id="四、其他命令"><a href="#四、其他命令" class="headerlink" title="四、其他命令"></a>四、其他命令</h1><h2 id="查看状态"><a href="#查看状态" class="headerlink" title="查看状态"></a>查看状态</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">status<br></code></pre></td></tr></table></figure><h2 id="查看帮助"><a href="#查看帮助" class="headerlink" title="查看帮助"></a>查看帮助</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs bash">table_help<br><span class="hljs-built_in">help</span><br></code></pre></td></tr></table></figure><h2 id="查看版本"><a href="#查看版本" class="headerlink" title="查看版本"></a>查看版本</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">version<br></code></pre></td></tr></table></figure><h1 id="五、参考链接"><a href="#五、参考链接" class="headerlink" title="五、参考链接"></a>五、参考链接</h1><p><a href="https://www.cnblogs.com/ityouknow/p/7344001.html">Hbase shell 命令介绍</a></p><p><a href="https://juejin.cn/post/6844903885937573901">漫谈HBase Filter</a></p><p><a href="https://www.361shipin.com/blog/1538688631781195776">HBase 使用过滤器</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;一、命令行登录方式&quot;&gt;&lt;a href=&quot;#一、命令行登录方式&quot; class=&quot;headerlink&quot; title=&quot;一、命令行登录方式&quot;&gt;&lt;/a&gt;一、命令行登录方式&lt;/h1&gt;&lt;p&gt;进入 hbase bin 目录下，执行以下命令：&lt;/p&gt;
&lt;figure class</summary>
      
    
    
    
    
  </entry>
  
</feed>
